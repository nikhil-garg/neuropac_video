link,Year,Event,Speaker,Organization,About ,Summary,Source of Summary,Topic 1,Sub-topic 1-1,Sub-topic 1-2,Sub-topic 1-3,Topic 2,Sub-topic 2-1,Sub-topic 2-2,Sub-topic 2-3
https://www.youtube.com/watch?v=KfLb7pJfFB4&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=1,2022,NICE,Stefeno Fusi,Columbia University,The pervasiveness of disentagled representational geometries in the brain. ,"The speaker is discussing ""disentangled representations,"" which are becoming popular in the machine learning community. They believe that these types of representations can be found in the real biological brain, and that by studying them, we can build better neuromorphic systems that are able to generalize. The speaker mentions that they have been working with a team of people who are studying disentangled representations in both animals and humans, and have developed new analytical tools to study them. They also provide an example of how these representations can be used to group similar stimuli together into categories, and discuss the concept of a ""mental representation"" of a category.",ChatGPT,Neuromorphic circuits,Network topology,,,Neuromorphic algorithms,Computational Neuroscience,,
https://www.youtube.com/watch?v=PuGe42HllHA&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=2,2022,NICE,Christopher Kymn,UCB,Computing on Functions using randomized vector representations,"The speaker, Christopher Kymn, is a PhD student at UC Berkeley and the Redwood Center for Theoretical Neuroscience. He is giving a talk at a conference about a paper he worked on called ""Computing on Functions using Randomized Vector Representations."" The paper presents a framework for computing over distributed representations, and the talk will focus on how to manipulate and transform vectors in meaningful ways in the original vector space. The talk will also discuss the potential applications of this framework in machine learning, factorizable image encodings, and modeling in neuroscience. The speaker mentions that this is a rapidly emerging field and that the framework, known as Vector Symbolic Architectures or Hyperdimensional Computing, can be linked to neuromorphic hardware.",ChatGPT,Neuromorphic algorithms,Stochastic computing,,,,,,
https://www.youtube.com/watch?v=LD0kf02opW8&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=3,2022,NICE,Denis Klekyo,UCB,Integer factorization with compositonal distributed representations,"The video is discussing a study on neuromorphic engineering, which is a challenging field when it comes to programming neural hardware. The speaker and his colleagues at Intel and UC Berkeley are exploring ways to streamline the process by drawing inspiration from Mars levels for information processing devices. They focus on the middle algorithmic level, where they believe that hyperdimensional computing or Vector Symbolic Architectures (VSA) can be a great abstraction for programming neuromorphic hardware. They also mention Vector Function Architecture (VFA), which is an enhanced version of VSA, and mention that it will play an essential role in the development of neuromorphic algorithms. They explain that VFA uses a random base vector and a similarity kernel to bind and superpose vectors, and that these operations have meaningful interpretations in the original space.",ChatGPT,Neuromorphic algorithms,Symbolic computing,,,,,,
https://www.youtube.com/watch?v=8xMWsXwAY9I&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=4,2022,NICE,Andreas Baumbach,Heidelberg University,Quantum many-body states: A novel neuromorphic application,"The speaker, Andreas Baumbach, is discussing a project that combines quantum physics and neuromorphic computing. He explains that the project began with a group of physicists at the same institute, including Tomas Gazenza, who wanted to represent quantum many-body states using the hardware developed by Calan Smiles. The speaker mentions that it took some time for the team to understand each other's language and for enough hardware to be available, but they eventually made progress. He also mentions that the project involves using neomorphic hardware, specifically the BrainScale's ASIC. The speaker also briefly mentions qubits, which are a type of quantum bit that can exist in a superposition of 0 and 1, and the concept of superposition is more complicated than a classical bit system.",ChatGPT,Applications,Quantum computing,,,,,,
https://www.youtube.com/watch?v=ThDh1XQqN4s&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=5,2022,NICE,Brad Aimone,Sandia National Laboratories,COINFLIPS: CO-designed improved neural foundations leveraging inherent physics stochasticity,"The researcher at Sandia National Labs is discussing a new effort called Coin Flips. The effort aims to explore the advantages of neuromorphic computing, which is a type of computing that is inspired by the human brain. In particular, the researcher is discussing a recent paper that was published in Nature Electronics, which focuses on how neuromorphic computing is advantageous for Monte Carlo random walk calculations. This is a probabilistic applied math algorithm, and it is thought to broaden the impact of neuromorphic computing as a whole. The researcher also explains that in the study, they designed a neuromorphic algorithm that uses spiking, leaking, integrate and fire neurons, and that the algorithm shows an energy advantage over traditional computing methods.",ChatGPT,Neuromorphic algorithms,Stochastic computing,Computing with physics,,,,,
https://www.youtube.com/watch?v=bn5pcFHPJHs&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=6,2022,NICE,Johannes Schemmel,Heidelberg University,BrainScaleS via EBRAINS: easily accesible analog,"The speaker is discussing the use of EBRAINs and BrainScaleS analog systems in brain emulation as part of the Human Brain Project. The goal is to make it easily accessible for neuroscientists and to hide the difficulties of working with analog models. The speaker is part of a research group that has been working on analog and mixed-signal systems for brain emulation since 1995, and is now focusing on the Human Brain Project, a European Union project aimed at understanding the brain and developing future computing technology. The speaker will also discuss the services that can be offered using this kind of system, and give an example of parameter fitting with BrainScales, which is important for brain emulation.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,Analog computing,,Tools and datasets,Computing platform,,
https://www.youtube.com/watch?v=Nfgr6XV8ORA&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=7,2022,NICE,Tobias Thommes,Heidelberg University,Demonstrating BrainScales-2 inter-chip pulse communication using EXTOLL ,"The speaker is discussing a demonstration of using BrainScales to inter-chip communication. They are emulating biological neurodynamics in analog electronic circuits and digitizing spike events from these analog circuits to communicate between neuron circuits in their chips and systems. They have an accelerated model dynamics compared to biology, with a speed up factor of 10 to the five in one system and 10 to the four in another. They also have embedded processors on their chips. However, they want to communicate neural events over a packet-based network to emulate larger neural networks, so they need a high-bandwidth network with low latency and small packets. They have found an alternative network, that offers bandwidths of up to 100 gigabits per second and latencies of 70 nanoseconds per hop. They are currently using 16 gigabits per sync per link and have a goal of communicating pulse events over the network to interconnect emulated networks on several chips.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,Communication,,Applications,HW Demo,,
https://www.youtube.com/watch?v=-UjtFAjinFQ&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=8,2022,NICE,Christian Mayr,TU Dresden,SpiNNaker 2: A platform for real-time bio-inspired AI and cognition,"The speaker is discussing SpiNNaker2 platform which he is working on with a team from Manchester. He takes an abstract view of neuromorphics, believing in the use of sparsity in coding to compress information and improve energy efficiency. He also mentions the use of prediction and parallel, asynchronous processing to make the system more robust and efficient. He also talks about the SpiNNaker1 and SpiNNaker2 systems, which are arm-based and have a high capacity and programmability. The goal is to merge brain inspiration and deep neural networks in all aspects of the system.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,,,,,,
https://www.youtube.com/watch?v=eFY1PhdmXsg&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=9,2022,NICE,Wolfram Pernice,Heidelberg University,Photonic neuromorphic processing,"The speaker is discussing their research on using optical principles for computing applications. They are building systems on chip that use optical waveguides to guide light signals across the chip and manipulate those signals with active components to implement information processing capabilities. They use an analog computing approach and the benefits of optics include low power and high throughput. The speaker also mentions that they primarily work with silicon photonics materials and use a phase change material called GST for programmability, which allows them to switch between glass-like and crystalline states quickly and reversibly. They work in both winstop and Heidelberg, and mostly do chip design and nano fabrication in the lab.",ChatGPT,Neuromorphic circuits,Photonics,,,,,,
https://www.youtube.com/watch?v=nG8USGLkoUc&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=10,2022,NICE,Kenneth Stewart,Accenture technology,Encoding event-based data with hybrid SNN guided variatinal auto-encoder in neuromorphic hardware,"Kenneth Stewart, a PhD student at the University of California, Irvine, discusses his work on encoding event-based data with a hybrid guided spiking neural network variation autoencoder neuromorphic hardware. He explains that supervised learning achieves state-of-the-art performance on a variety of tasks, but it requires labeled data along with many iterations of training. He suggests using the power of supervised learning but being able to learn from enable data, such as with variational autoencoders, which can learn a disentangled representation of data such as gesture data to be able to learn from new, unlabeled data. He explains the architecture of his hybrid guided variational autoencoder, which uses a spiking neural network encoder and an unspiking convolutional neural network decoder, and the goal is to embed streams of dynamic vision sensor events into a latent space to facilitate the evaluation and data similarity for self-supervised learning from real-world data. He focuses on demonstration of the guided VA on a dynamic vision sensor learning problem using the event-based DVS gesture data set.",ChatGPT,Neuromorphic algorithms,Computing with spikes,Spike encoding,,,,,
https://www.youtube.com/watch?v=QAkeSpCk7uY&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=11,2022,NICE,Dhireesha Kudithipudi,UTSA,Lifelong learning systems inspired by neural plasticity,"The speaker is discussing the concept of ""lifelong learning systems"" in artificial neural networks, and the challenges that come with designing such systems. The speaker notes that current artificial neural networks tend to ""forget"" what they have previously learned when presented with new tasks, and that a lifelong learning system should be able to rapidly adapt to new conditions without forgetting previous knowledge. The speaker also notes that the system should be able to continuously improve its performance while executing previously learned tasks and when new tasks are introduced. The speaker goes on to explain the six key features they have identified as necessary for designing a lifelong learning machine, including transfer and adaptation, overcoming catastrophic forgetting, and more.",ChatGPT,Neuromorphic algorithms,Learning in SNNs,,,,,,
https://www.youtube.com/watch?v=cHsFGoXNzWI&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=12,2022,NICE,Michael Hopkins,University of Manchester,BitBrain and sparse binary coincidence (SBC) memories,"The speaker is discussing an idea called ""Bit Brain,"" which is a memory mechanism that uses ideas from sparse coding, computational neuroscience, information theory, and other fields to create a low energy, low latency product for learning and inference. The primary focus of the idea is for use in internet of things applications, and the goal is to create a system that can quickly and efficiently learn and adapt in a continuous manner. The speaker notes that they and a colleague filed a patent on the key elements of the idea, and that this is the first public talk about the idea without a non-disclosure agreement in place. They also mention an example implementation of the idea called an ""address decoder element"" which helps to illustrate the concept.",ChatGPT,Neuromorphic algorithms,Memory,,,,,,
https://www.youtube.com/watch?v=FuLElIzIdWo&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=13,2022,NICE,James Turner,University of Sussex,Event-based datasets for classification and pose estimation,"The speaker is discussing a method for collecting event-based data sets for pose estimation using artificial neural networks. They mention that there are relatively fewer data sets for spiking neural networks, and they are trying to develop a human helper for handling manipulating tools and passing them to a human subject. They are using a set of eight Vicon cameras for 3D tracking and a set of two DVS cameras for event-based vision. The problem they have encountered is that the Vicon system's infrared strobe light causes a saturated frame on the DVS cameras, so they have developed a solution by custom 3D printing props with hollow insides and low power infrared tracking LEDs. They are also using a flashing marker to record on the Vicon system and the DVS cameras in order to compute the best rotation and translation from Vicon space into 3D camera centric space. They have collected a sample data set and made it available online.",ChatGPT,Tools and datasets,Dataset,,,Applications,Event vision sensors,,
https://www.youtube.com/watch?v=aIKQ5r0yn6k&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=14,2022,NICE,Connor Bybee,UCB,Optimal oscillator memory networks,"The video is a talk by a graduate student named Connor Bybee at the Redwood Center for Theoretical Neuroscience at UC Berkeley. He presents work on ""optimal oscillator memory networks"" or ""optimal oscillator associative memories,"" which connect algorithms and hardware in the field of attractor networks. These networks are important in neuroscience and are becoming increasingly important in machine learning. The talk focuses on auto-associative or recurrent networks, which have feedback, and their implementation in oscillator networks. He also discusses issues such as designing efficient associative memories for neuromorphic hardware and the compatibility of algorithms with different hardware platforms.",ChatGPT,Neuromorphic circuits,Spiking neural networks,Network topology,,Neuromorphic algorithms,Memory,Computing with physics,
https://www.youtube.com/watch?v=FH976Dg4RWA&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=15,2022,NICE,Samuel Schmidgall,George Mason University,Scale lifelong learning: Spiking neurons as a solution to instability in plastic neural networks,"The speaker, Samuel Schmidgall, is presenting his work on using spiking neurons as a solution to instability in plastic neural networks. He is discussing the difference between artificial and spiking neurons, with the latter accumulating information across the time domain through membrane potential and a spike. He also explains the concept of synaptic plasticity, which is the basis for many learning algorithms in AI, and the specific plasticity rules (such as spike timing dependent plasticity rules) that have been used in previous works. He then goes on to discuss the use of evolutionary strategies as a optimization technique for non-differentiable models of spiking neurons, which allows for updating both the plasticity parameters and the initial weights to learn the synaptic plasticity parameters.",ChatGPT,Neuromorphic algorithms,Learning in SNNs,,,,,,
https://www.youtube.com/watch?v=rC_yerhgvL4&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=16,2022,NICE,Daniel Gutierrez Galan,University of Seville,Towards the neuromorphic implementation of auditory perception in the iCub robotic platform,"The speaker, Daniel Gutierrez, is from the University of Sevilla and is presenting work he has done in collaboration with the group of Kiara Bartolozzi from the Italian Institute of Technology. The project is focused on implementing a neuromorphic cochlear model on the iCubrobot, which is a robotic platform. The main motivation for the work is to provide the iCub robot with the sense of hearing, as it currently has vision and touch but not hearing. The speaker explains the auditory extended pathway and how it works from a biological perspective, specifically focusing on the cochlea and its role in decomposing sound and organizing frequencies. The speaker also mentions that this is still a work in progress and they will show preliminary results and discuss improvements and future work that needs to be done.",ChatGPT,Applications,Robotics,Neuromorphic audition,,,,,
https://www.youtube.com/watch?v=Q4QyX-jHn6U&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=17,2022,NICE,Madeleine Abernot,CNRS,Oscillatory neural network as hetero-associative memory for image detection,"In this video, Melinda Bennett from Zulium Laboratory in Montpellier, France presents work done on using oscillatory neural networks (ONNs) as an associative memory for image edge detection. ONNs are networks of coupled oscillators that use an analog-based computing paradigm, where each neuron is emulated with an analog oscillator and the oscillators are coupled with analog components. The information is represented in the phase relationship between the oscillators. The work in the Neuron project explores using ONNs as a heterogeneous associative memory, where an input is associated with an output, rather than a fuzzy pattern with a clear pattern. They also show how ONNs can be used to perform edge detection, which is a commonly used technique in image processing. They replace the traditional convolutional filters used in edge detection with ONNs as an associative memory, which is able to detect edges in an image.",ChatGPT,Neuromorphic circuits,Network topology,Spiking neural networks,,Neuromorphic algorithms,Memory,Computing with physics,
https://www.youtube.com/watch?v=jomxC46hOOE&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=18,2022,NICE,Jeff Clune,University of British Columbia University,Materials Matter: How biologically inspired alternatives to conventional neural networks improve meta-learning and continual-learning,"In this video, the speaker discusses the idea that materials matter in AI and how looking to biology can inspire alternatives to conventional neural networks that can improve our ability to do meta learning. They also mention a specific study they conducted on the evolution of structural organization in neural networks, and how adding a connection cost in addition to selection for performance can lead to more modular networks that are better at adapting to new situations. They also mention the concept of metalearning and the two major camps in this field, one being the ""mammal camp"" which focuses on optimizing initial weights and the other camp focuses on an algorithm that can learn to learn over time.",ChatGPT,Neuromorphic algorithms,Learning in SNNs,Learning to learn for neuromorphic HW,,,,,
https://www.youtube.com/watch?v=3iMx4ZFAP8o&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=19,2022,NICE,Jack Lindsey,Sandia National Laboratories,Sequence learning and consolidation on loihi using on-chip plasticity,"The video is discussing work being done by a PhD student in neuroscience at Columbia University in collaboration with the neuromorphics group at Sandia. They are using on-chip plasticity to model sequence learning and consolidation using a programmable local learning rule on the Loihi chip. They are using a toy environment consisting of a finite set of stimuli presented in a streaming fashion in a set of stereotype sequences. The goal is to make predictions, represented as activity patterns in a separate population of spiking neurons, about the upcoming stimulus based on the sequential structure that has been observed previously. They are motivated by the hippocampus, a region of the brain involved in sequence learning and memory, and the proposed mechanism for this kind of learning is Spike timing independent plasticity (STDP). They are using Loihi chip to implement a synaptic plasticity rule that implements STDP. They are stimulating the neurons corresponding to the first pattern in one of these sequences and the network will rehearse the rest of the sequence, making predictions about future sequence elements. The issue is that the learning rule is storing this kind of predictive structure in recurrent weights in a population of neurons, but ultimately in an online setting, they would like to make predictions in real-time based on the current input.",ChatGPT,Neuromorphic circuits,On-chip learning,Learning in SNNs,,,,,
https://www.youtube.com/watch?v=FVwXYCVkmbY&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=20,2022,NICE,Adam Perrett,University of Manchester,Online learning in SNNs with e-prop and neuromorphic hardware,"The speaker is discussing online learning in neuromorphic systems, specifically focusing on the eprop algorithm and how it can be used in conjunction with neuromorphic hardware such as SpiNNaker. The motivation for this work is the ability for the system to continuously learn and update while interacting with the environment, as well as the benefits of low power and latency that neuromorphic systems offer. The speaker also briefly mentions how memory works in neural networks, specifically referencing LSTMs and the challenges that come with using them for online learning. Overall, the video is discussing the use of specific algorithms and technology to improve the ability of systems to learn and adapt in real-time while also being efficient in terms of power and computation.",ChatGPT,Neuromorphic algorithms,Learning in SNNs,On-chip learning,,,,,
https://www.youtube.com/watch?v=GaAbwbfZA7c&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=21,2022,NICE,Diego Chavez Arana,New Mexico State University,A neuromorphic normalization algorithm for stabilizing synaptic weights with application to dictionary learning in LCA,"The video describes a proposed solution for stabilizing synaptic weights in neuromorphic machine learning implementations. The solution involves addressing the concept of ""heavy learning,"" in which the strength of synapses between neurons increases when the neurons are activated simultaneously. Over time, this can cause stability problems. The proposed solution is a neuromorphic algorithm that directly normalizes synaptic connectivity with the neural circuit using a simfire gated sim-for change based information control network which works in concert with heavy learning. The algorithm uses a locally competitive algorithm (LCA) and gradient descent to minimize the cost function and converge the synaptic connectivity values to a unique length asymptotically. The information flow in the network is controlled by synfire gated neurons.",ChatGPT,Neuromorphic algorithms,Learning in SNNs,,,,,,
https://www.youtube.com/watch?v=VTvWnZlBdNo&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=22,2022,NICE,James Knight,University of Sussex,Efficient GPU training of SNNs,"James Knight is discussing the work he have been doing over the last year on efficiently training spiking neural networks (SNNs) on GPUs. He explains that there is a long history of simulating SNNs, but most simulators focus on simulating single instances of these models and were traditionally run on distributed CPUs. These simulators are not the tools that people want to use for machine learning anymore. Neuromorphic hardware has the potential to save a lot of energy, but online learning on these systems is still challenging, and current gradient-based learning algorithms are very data-hungry. People have started using standard machine learning tools like PyTorch, TensorFlow and JaX to GPU-accelerate SNN research. Knight argues that this is not a good way of representing an SNN as it uses a global memory between each operation, which slows things down further and scales with the depth of the model. He suggests Jen as an alternative which focuses on computational neuroscience and neurobotics applications, and allows for maximum control, making it a better choice for custom stuff.",ChatGPT,Neuromorphic circuits,Spiking neural networks,,,,,,
https://www.youtube.com/watch?v=uI6sCJPc_aY&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=23,2022,NICE,Paul Haider,University of Bern,Latent equilibrium: A unified learning theory for arbitrarily fast computation with arbitrarily slow neurons,"The speaker, Paul Haider, is a PhD student in the newer TMA group at the University of Bern. He is presenting on ""latent equilibrium,"" which is a unifying framework for arbitrarily fast computation and learning with arbitrarily slow neurons. The talk is about efficient information processing in dynamical physical systems, such as the human brain. One of the key challenges is understanding learning in such systems, which can be translated to the credit assignment problem. In the case of artificial neural networks, this is done with the backpropagation algorithm, but since this algorithm is not biologically plausible, there are a lot of bioplausible approximations, such as equilibrium propagation, the segregated dendrites model, and the dendritic cortical micro circuits. However, the speaker argues that all of these models have a common problem, which is that their speed of information processing is limited by the speed of their components, which for neural networks are the neurons themselves. The talk also covers how this slowness affects the dynamics of networks and learning in these systems.",ChatGPT,Neuromorphic algorithms,Learning in SNNs,,,,,,
https://www.youtube.com/watch?v=uX-BiOOQC8E&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=24,2022,NICE,Pau Vilimelis Aceituno,ETH Zurich,Information theory limits of neuromorphic energy efficiency,"The speaker is discussing how to use information theory and energy efficiency to design neuromorphic systems, which are efficient in some specific problems. They mention that the systems are heterogeneous and it can be difficult to know how efficient they can be or how to design them. The speaker references Shannon's theorems, which state that the right language for information is to use symbols and probabilities and that reliable communication or computation can work even if there is noise on the hardware. The speaker is proposing a new approach to Shannon's theorems by considering costs that are not symmetric and using a combinatorial view to encode symbols with the fewest number of neurons and active neurons to minimize costs. They also mention that this approach is similar to a classical paper on neural computation, but instead of trying to infer efficiency values, they are trying to think of it as a design problem.",ChatGPT,Neuromorphic algorithms,Symbolic computing,,,,,,
https://www.youtube.com/watch?v=6eNIMr_u61o&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=25,2022,NICE,Johannes Leugering,Fraunhofer-Gesellschaft,Modeling and analyzing neuromorphic SNNs as discrete event systems,"The speaker, Johannes Leugering, is discussing a hobby project he is working on related to the simulation of spiking neural networks (SNNs) and discrete Event Systems. He explains that while detailed biophysical models of neurons, such as the Hodgkin-Huxley model, are good at capturing the biological properties of neurons, they are difficult to simulate on a large scale and it can be difficult to understand how they solve problems. He notes that many in the field of neuromorphic computing and SNNs have moved to simpler neural models, such as the leaky integrate-and-fire neuron model, which retain important features of the biological models but are simpler to simulate. He expresses that a drawback of this approach is that it is difficult to explain how these networks actually do computation, with the most common mental model being the concept of function approximation. He is interested in describing computation in an event-driven way, and is motivated by the idea that neurons may primarily perform coincidence detection, which can be captured by simpler models such as finite state machines or Petri nets.",ChatGPT,Neuromorphic algorithms,Computational Neuroscience,,,Neuromorphic circuits,Spiking neural networks,,
https://www.youtube.com/watch?v=GJKZruFuYw0&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=26,2022,NICE,Catherine Schuman,University of Tennessee,A framework to enable top-down co-design of neuromorphic systems for real-world applications,"The speaker is discussing a framework for top-down co-design of neuromorphic systems for real-world applications. The framework, developed by the speaker's research group at the University of Tennessee, aims to bridge the disconnect between ongoing research efforts in neuromorphic hardware and the needs of applications and algorithms in the neuromorphic computing space. The framework includes a library of input and output coding approaches and common interfaces to back-end neuromorphic processors, as well as simulation frameworks for architecture assessment. The speaker's group also uses FPGA platforms and in-house built simulators to evaluate different neuromorphic hardware designs. The goal is to make it easier to understand how neuromorphic hardware can be useful in different applications and to help researchers easily extend the framework to support new neuromorphic hardware efforts.",ChatGPT,Neuromorphic circuits,Emerging hardware approaches for optimization,Network topology,Spiking neural networks,Neuromorphic algorithms,Evolutionary optimization,,
https://www.youtube.com/watch?v=1IqtUg-8rcc&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=27,2022,NICE,Felix Wang,Sandia National Laboratories,Localization through grid-based encodings on digital elevation models,"The speaker, Felix Wang, is presenting a neuro-inspired localization algorithm that he has been developing in collaboration with colleagues at Sandia National Labs. The problem they are addressing is sensor-based localization, which is the process of determining an agent's position based on sensor input. They are implementing a distributed algorithm based on neuro-inspired models of grid cells and are applying it to a navigation-based dataset, specifically digital elevation models. They have adapted the grid-based representations to an abstract computational model and have refined it by aggregating the grid cells into modules that share the same spatial scale and orientation. They have also highlighted that using this distributed phase code representation has nice properties such as robustness to coding noise and the ability to represent thousands of distinct grid modules.",ChatGPT,Applications,Robotics,Collaborative autonomous systems,,,,,
https://www.youtube.com/watch?v=_PNlb126vdU&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=28,2022,NICE,Craig Vineyard,Sandia National Laboratories,Neural mini-apps as a tool for neuromorphic computing insight,"The speaker is discussing the challenges of identifying a ""neuromorphic advantage"" in the context of benchmarking for novel spiking algorithms, and how traditional benchmarks are defined by the processing to be executed or the data set and task. They argue that this flexibility makes direct comparisons difficult, and that many data-driven machine learning benchmarks fall short in their ability to convey merit for ongoing comparisons. The goal is not to create yet another benchmark, but to advance the field and gain understanding of how to make benchmarks more useful for ongoing comparisons.",ChatGPT,Tools and datasets,Benchmark,,,,,,
https://www.youtube.com/watch?v=UM1mhq1bdY4&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=29,2022,NICE,Eric Muller,Heidelberg University,Hands on tutorial BrainScaleS 2: Introduction,"BrainScaleS 2 is a neomorphic system, which means it uses analog circuits to model neuron behavior in a physical model. The system is an infrastructure project and is designed to support a variety of use cases. The hardware includes a simplified neural model, short-term plasticity, structured neurons, accelerated model dynamics, and programmable plasticity. The system has multiple types including a lab setup and an embedded system, with the latter being designed for mobile operation. The BrainScaleS 2 tutorial video is an introduction to the software and hardware of the system, as well as information on how to access and use the system.",ChatGPT,Tools and datasets,Computing platform,Tutorial,,Neuromorphic circuits,Large scale neuromorphic computing platform,,
https://www.youtube.com/watch?v=907zRDofz7Y&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=30,2022,NICE,Jacob Kaiser,Heidelberg University,"Hands on tutorial BrainScaleS 2: Structured Neurons
    ","The video is a tutorial on using the BrainScaleS 2 system to create neural morphologies using structured neurons. The tutorial focuses on creating a simple chain of compartments using the system's ability to connect several neuron circuits and make use of the capabilities of each neuron circuit, such as passive properties like PSP propagation and active properties like dendritic spikes. The tutorial also covers how to establish connections between the different circuits, such as horizontal connections and a shared line, and how to form various neuromorphologies. The tutorial also shows how to use the system to perform an experiment where a synaptic input is injected into one compartment and the resulting PSP is recorded in different compartments, and how to plot the results.",ChatGPT,Tools and datasets,Computing platform,Tutorial,,Neuromorphic circuits,Large scale neuromorphic computing platform,,
https://www.youtube.com/watch?v=TkkIR7zhwMI&list=PLJ506hQ4g3TiJnL5_UFtRgDFcEV9QNdd1&index=31,2022,NICE,Andrew Rowley,University of Manchester,Hands on tutorial SpiNNaker,"The speaker is going through a tutorial on using the SpiNNaker system, which is a computer architecture designed for running simulations of neural networks. They explain how to access the system using the link provided by Bjorn, which leads to a co-laboratory interface where users can access the system and run simulations using the Pine programming language. The speaker also mentions that there is an older URL for accessing the system directly, but the co-laboratory is the recommended way of accessing it. They also mention that the system uses the Jupiter lab interface, and that it has the Spinnaker and Spinnaker kernels installed, which are based on Python 3. They also mention that there is a work folder that can be used to store files permanently, and that the system is generally not reset without the user's request.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,,,Tools and datasets,Computing platform,Tutorial,
https://www.youtube.com/watch?v=-dl1FPrpw1A&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=1,2021,NICE,Mike Davies,Intel ,Lessons from Loihi for the future of neuromorphic computing,"The speaker, Mike Davies, is giving an update on the progress of the Loihi program at Intel, which is focused on creating intelligent, adaptive, and agile drones. He notes that the field of drone racing has advanced quickly in recent years, but that there is still a large gap in capabilities between what is currently possible with autonomous drones and what is possible with even the simplest animal brains, such as a parrot. The speaker suggests that by emulating the efficiency and speed of these simple brains, it may be possible to create drones that are much more capable and efficient than what is currently possible.",ChatGPT,Neuromorphic circuits,ASICs,,,,,,
https://www.youtube.com/watch?v=5UBbCNQW_7c&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=2,2021,NICE,Ryad Benosman,UPITT,Why is neuromorphic event-based engineering the future of AI?,"The speaker, Ryad Benosman, is discussing the field of neuromorphic processing and why it is considered to be the future of AI. He references a group of people who began thinking about alternative forms of processing in the 1990s, and how advancements in sensors and processors have led to the current interest in neuromorphic processing from big technology companies. He argues the overwhelming amount of data being produced, is not sustainable and that neuromorphic processing offers a solution with its low power and low latency capabilities. The speaker also mentions that traditional methods of using CPUs or GPUs to process data from event-based cameras is not the most efficient way to handle the data, and that neuromorphic processing offers a better solution.",ChatGPT,Generic Introduction,,,,,,,
https://www.youtube.com/watch?v=y4OJLJYCmGU&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=3,2021,NICE,Johannes Schemmel,Heidelberg University,Exploring the possibilities of analog neuromorphic computing with BrainScales,"It appears that the speaker is discussing the possibilities and advancements in computing technology, specifically in regards to energy consumption and performance. They mention specific companies such as Google and Tesla, and mention the importance of understanding logical information processing and future computing based on what is currently being learned. They also mention the use of specific technologies such as GPU's from Nvidia and IBM Datacenter. It also mentions some other areas such as green electronics systems, quantum computing, and machine learning. The overall theme is improving efficiency and performance of computing technology through advancements in research and development.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,Analog computing,,,,,
https://www.youtube.com/watch?v=iRVo08j7sDM&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=4,2021,NICE,Sebastian Schmitt,Heidelberg University,BrainScaleS-1 From clean room to machine room,"The speaker is discussing the development and use of physical modeling of analog neuron circuits for building neuromorphic systems in Heidelberg. The approach uses dedicated circuits in every neuron to mimic the functions of biological neurons, and results in accelerated dynamics compared to real-time biology. They also mention the use of wafer scale integration to connect multiple chips together to emulate larger neural networks, and the use of a brain scales wafer module to house the wafer, distribute power, and handle input and output communication. The speaker also mentions the steps taken to assemble and test these systems, including optical inspection of the wafer.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,Analog computing,,,,,
https://www.youtube.com/watch?v=hVu6Hi7Dktk&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=5,2021,NICE,Korbinian Schreiber,Heidelberg University,Closed-loop experiments on BrainScales-2,"The speaker is discussing the BrainScales2 platform, which is a chip that contains a hybrid architecture of an analog or mixed signal neural network core and a digital CPU. The chip has a high configurability and can be used for a variety of experiments and code implementation. The speaker also mentions that the BrainScales2 chip has a speed-up factor of 1000, which presents both advantages and challenges for researchers studying phenomena such as developmental processes in brains. The faster time-scales can greatly improve experiment throughput, but they can also be harder to handle.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,Analog computing,,,,,
https://www.youtube.com/watch?v=RtYahwyless&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=6,2021,NICE,Jonathan Tapson,University of Technology Sydney,Batch << 1: Why neuromorphic computing architectures suit real-time workloads,"In his talk, Jonathan Tapson, the former Chief Science Officer of Gray Matter Labs, discusses the lessons learned from building a neuromorphic architecture for commercial real-time computing. He argues that neuromorphic architectures are suitable for a particular kind of machine learning and that the focus should be on specific domains in which they excel, rather than claiming more general advantages. He also emphasizes the importance of considering the specific workloads that the architecture will be used for, as edge workloads involve real-time decision-making and are different from cloud workloads. He points out that the appropriate architecture for processing uncorrelated data is different from that for processing highly correlated data.",ChatGPT,Generic Introduction,Uncategorized,,,,,,
https://www.youtube.com/watch?v=u5kLjflMslY&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=7,2021,NICE,Thomas Pfeil,Bosch center for AI,Neuromorphic and AI research at BCAI,"Thomas Pfeil is a researcher at the Bosch Center for Artificial Intelligence, which is mainly located in Renningen, Germany. He is discussing research related to neuromorphic computing and the publicly funded project ulpec. Bosch is a large company with 400,000 employees and focuses on mobility solutions, with machine learning being a major area of research for the company. Thomas mentions that Bosch is participating in the ulpec project, which is about creating a smart microsystem that interconnects an event-based camera with a spiking neural network. The goal of this project is to develop low power, low latency, and event-driven systems. Thomas mentions some of the challenges in this field, including training large-scale networks and finding appropriate data sets.",ChatGPT,Generic Introduction,Uncategorized,,,,,,
https://www.youtube.com/watch?v=fjG_0zKGVfw&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=8,2021,NICE,Florian Kelber,TU Dresden,Mapping deep neural networks on SpiNNaker2,"In this video, the speaker, Florian Kelber, presents his work as a PhD student at the Technical University of Dresden, where he has adapted the Spinnaker 2 architecture to improve performance for non-spiking neural networks. He also explains how specific models have been mapped onto the Spinnaker 2 architecture, and how the team is trying to maximize performance and energy efficiency by including an accelerator for matrix multiplication and deconvolution. The Spinnaker 2 chip is set to be released in the next one and a half to two months, with plans to build a 5 million core machine in 2022. Overall, the focus is on creating hybrid neural networks architectures and improve the performance for the specific tasks.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,,,,,,
https://www.youtube.com/watch?v=t53zEJLs39U&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=9,2021,NICE,Jeff Hawkins,Numenta,From brains to silicon: Applying lessons from neuroscience to machine learning,"In this video, the speaker is discussing the work of their company, Numenta, which aims to reverse engineer the neocortex and apply what they learn to AI and machine learning. The speaker explains that the neocortex is responsible for all forms of intelligence, including high-level sensory perception, motor behaviors, language, and abstract thoughts. They also mention that the neocortex has the ability to continuously learn and that the basic unit of computation in the neocortex is the cortical column, which is a complete modeling system that works on implementing a type of reference frame. They also explain that the neocortex creates a model of the world in our heads, allowing us to infer our current situation and generate goal-oriented behavior.",ChatGPT,Neuromorphic algorithms,Computational Neuroscience,,,,,,
https://www.youtube.com/watch?v=BnRpZNuXgME&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=10,2021,NICE,Brad Aimone,Sandia National Laboratories,A neuromorphic future for classical computing tasks,"The speaker is discussing neuromorphic computing and its potential impact on the broader computing world. They mention the concept of quantum computing, which has a clear benchmark for success called ""quantum supremacy"" that demonstrates its superiority over traditional computers. However, neuromorphic computing currently lacks a concrete benchmark for success, and the speaker proposes the idea of ""neuromorphic advantage"" as a way to measure the potential of the technology. This would involve identifying algorithms that are already advantageous on one axis (such as time or energy efficiency) and have strong scaling implications for the other axis. The speaker also mentions that they are focusing on current neuromorphic platforms such as the BrainScaleS and SpiNNaker systems, and comparing them to GPUs. They also note that future technologies such as analog and crossbar architectures will also be important to consider in the future.",ChatGPT,Generic Introduction,Uncategorized,,,Tools and datasets,Benchmark,,
https://www.youtube.com/watch?v=7yYfoLZA5Os&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=11,2021,NICE,Christoph Ostrau,Bielefeld University,Benchmarking of neuromorphic hardware systems,"The speaker is a PhD student from Beatlefield University, and the talk is about benchmarking new morphic hardware systems. The main question guiding their research is how to compare the performance and efficiency of diverse systems. They have developed a ""black box"" approach, called the Neomorphic Benchmark Approach, which takes the user's perspective and has a high-level description of a spiking neural network that is mapped by an abstraction layer to various target systems. This allows for direct cross-platform networks and a back-end independent implementation. However, due to the large variety of systems, they have to open the black box a bit to allow platform-dependent configuration of these benchmarks. The framework they are using is called Snapseed, and they are comparing modern CPU and GPU implementations to older technology used in the spinning cast by Game Brainscape Systems. They have different levels of benchmarks, starting with low-level characterization benchmarks and ending with full applications, to be representative of the field.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=KyFZKDT9uK8&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=12,2021,NICE,Andrew Sornborger,Los Alamos National Lab.,Implementing backpropagation for learning on neuromorphic spiking hardware,"Andrew Sornborger, from Los Alamos National Laboratory, and his team have developed a method for implementing backpropagation, a widely-used algorithm for training neural networks, on neuromorphic spiking hardware. They were able to train a three-layer network to classify digits from the MNIST dataset, achieving a validation accuracy of 92% and a throughput of 1000 frames per second. Additionally, the energy consumption per sample was 0.3 millijoules, and the energy delay product was 0.3 microjoule-seconds.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=4Hhn7GioCMo&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=13,2021,NICE,Fabian Sinz,University Gottingen,Inductive bias transfer between brains and machines,"The speaker, Fabian Sinz, is the head of the Neural Intelligence Group at the University of Tübingen and is moving to the University of Göttingen this summer. He is interested in understanding why biological neurons are so much better at certain tasks than current state-of-the-art machine learning algorithms. He discusses the success of deep learning in the last 10 years, specifically in computer vision, but notes that these algorithms are brittle and do not generalize well beyond the statistics of their training set. He uses the example of adversarial examples, where small, invisible perturbations can completely throw off a classifier's decision, and notes that this shows that these classifiers use different decision strategies compared to humans. He also discusses a study by Robert Garros that shows the robustness of humans compared to classifiers in the face of image distortion.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=pKNN93mNuc0&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=14,2021,NICE,Pau Aceituno,ETH Zurich,STDP leads to efficient coding of predictions,"The speaker is discussing research they did during their PhD at the Max Planck Institute for Mathematics and the Sciences. The talk is focused on computational neuroscience and the idea that learning is not just about adjusting weights, but also how the timing of spikes changes as an individual becomes more familiar with a certain input. They mention the concept of latency reduction, in which a postsynaptic spike advances in time as a neuron learns. They also mention some technical considerations, such as making sure depression is slightly larger than potentiation to prevent instability, and the potential for one postsynaptic spike to be weaker than another if they get too close together in time. The speaker also mentions the study of triplet rules as related to their research.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=gYMRy8ptbKk&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=15,2021,NICE,Andreas Wild,Intel,Intel Loihi's NxSDK: Introduction and overview,-,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=MrA2f15oVts&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=16,2021,NICE,Gabriel Andres Fonseca,Intel,A fast and efficient constraint satisfaction solver on Loihi,"The speaker is discussing the use of constraint satisfaction solvers, which are a type of problem that can be difficult to solve using traditional architectures. They mention that these problems are ""NP-complete,"" meaning they are interactive and decidable, but can be challenging to scale. The speaker suggests using neuromorphic hardware to solve these problems, as it is built for this purpose and has advantages such as fine parallelism, event-driven characteristics, and integrated compute and memory. They also provide a definition of a constraint satisfaction problem, which involves a set of variables that can take on specific values within a given domain, and restrictions on the values that these variables can take.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=k0NeUCMLjgY&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=17,2021,NICE,Sumit Bam Shrestha,Intel,SLAYER for Loihi,"The speaker is discussing the use of ""spike-based"" deep neural networks for low energy computing, specifically using the tool ""SLAYER"" for configuring these networks. They provide a background on why computing with spikes is useful, as well as an overview of the temporal interactions that occur in a spiking neuron. They mention that the goal of using these networks is to achieve low energy systems, and mention that SLAYER has a custom implementation for learning both synaptic weights and axonal delays. The speaker also mentions that they will be showing demos of training and live inference using these networks, as well as benchmark results.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=7WSaoFON25Q&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=18,2021,NICE,Garrick Orchard,Intel,Performance characterization on Loihi,"The speaker, Garrett Orchard, is a member of Intel's Neuromorphic Computing Lab. He is discussing the importance of benchmarking and characterization for the Luihi chip, which is a neuromorphic processor developed by Intel. He explains that benchmarking and characterization are important for understanding the capabilities of the technology, and for making claims that are backed up by data and measurements. He also notes that benchmarking can be used to compare the performance of Luihi to other technologies, and to provide a realistic picture of the chip's strengths and weaknesses. He also mentions that he will be running a notebook that INRC members can use to run these tests.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=Hk95LdvkQ4s&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=19,2021,NICE,Andrew Rowley,University of Manchester,SpiNNaker Tutorial,"The speaker is giving a tutorial on how to use the Spinnaker system, which can be accessed through a Jupyter notebook. The speaker explains that the server is located in Manchester and that users should use their ebrain's credentials to log in. The speaker also mentions that the tutorial will be conducted on the Jupyter lab interface, and that there are folders set up within the Jupyter system for users to save their work in, and also tutorials available. The speaker also mention that there is a link to the e-brains drive for users to store files for a longer period of time. The speaker also mentioned that even individuals not working at the company can get HBP credentials and that it is possible, but the process is a bit more time consuming.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,,,Tools and datasets,Computing platform,Tutorial,
https://www.youtube.com/watch?v=_WiFvEpKNuI&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=20,2021,NICE,Sebastian Billaudelle,Heidelberg University,BrainScaleS-2 Tutorial,"The speaker is discussing a tutorial on using ASICs (analog integrated circuits) for performing machine learning tasks. They mention that participants will have access to some of these ASICs and that the tutorial will cover two main topics: spiking neural networks and matrix multiplications with analog neural mobile hardware. They mention that the tutorial will include a demonstration of a single neuron and a learning rule called ""super spike."" The tutorial will also use a Jupyter notebook setup and a micro scheduler called ""quickly"" to optimize and interleave experiments on the available hardware. They also mention a list of publications that can serve as inspiration for the tutorial.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,,,Tools and datasets,Computing platform,Tutorial,
https://www.youtube.com/watch?v=KN-xhNiuAV4&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=21,2021,NICE,Johannes Weis,Heidelberg University,BrainScaleS-2 Tutorial,-,,Neuromorphic circuits,Large scale neuromorphic computing platform,,,Tools and datasets,Computing platform,Tutorial,
https://www.youtube.com/watch?v=XAlLdLtMWEI&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=22,2021,NICE,Arne Emmel,Heidelberg University,BrainScaleS-2 Tutorial,"The video discusses using the high-level API for matrix multiplication in hx torch, an extension to the PyTorch machine learning framework. The speaker imports various packages including numpy and torch, and demonstrates the use of the map mode operation for matrix multiplication. They mention that the operation can be multiplexed for larger matrices and that the hardware connection and calibration must be initialized before use. The speaker also mentions that the numbers used in the demonstration are in units of the analog-to-digital converter and that the result is returned as a PyTorch tensor.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,,,Tools and datasets,Computing platform,Tutorial,
https://www.youtube.com/watch?v=PNZzf53Ca6I&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=23,2021,NICE,Wolfgang Maass,TU Graz,Biological inspiration for improving computing and learning in spiking neural networks,-,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=jjbKwrmGX3E&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=24,2021,NICE,Jakob Jordan,University of Bern,Conductance-based dendrites perform reliability-weighted opinion pooling,"The speaker is discussing a new theory of single neuron computation that centers around the principle of integration. They observe that neurons in the cortex use this principle to integrate information from different sources, such as receptive fields or multiple modalities. They propose that single neurons use conductance-based synapses to accomplish this integration. They use the example of Bayesian inference to explain how neurons integrate information from different sources, with the posterior distribution being a weighted average of the original distributions. They also discuss the stationary state of bi-directional voltage dynamics in multi-compartment neural models, and explain how the membrane potential in the somatic compartment is a weighted linear combination of the dendritic potentials, with the weighting factors being the conductances generated on the dendrites.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=9xiQ-wOWhW8&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=25,2021,NICE,Elena Kreutzer,University of Bern,Natural-gradient learning for spiking neurons,"In many normative theories of synaptic plasticity, weight updates implicitly depend on the chosen parametrization of the weights. This problem relates, for example, to neuronal morphology: synapses which are functionally equivalent in terms of their impact on somatic firing can differ substantially in spine size due to their different positions along the dendritic tree. Classical theories based on Euclidean gradient descent can easily lead to inconsistencies due to such parametrization dependence. The issues are solved in the framework of Riemannian geometry, in which we propose that plasticity instead follows natural gradient descent. Under this hypothesis, we derive a synaptic learning rule for spiking neurons that couples functional efficiency with the explanation of several well-documented biological phenomena such as dendritic democracy, multiplicative scaling and heterosynaptic plasticity. We therefore suggest that in its search for functional synaptic plasticity, evolution might have come up with its own version of natural gradient descent.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=qLaq1m0xVuQ&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=26,2021,NICE,Johannes Leugering,Fraunhofer IIS,Making spiking neurons more succinct with multi-compartment models,"Spiking neurons consume energy for each spike they emit. Reducing the firing rate of each neuron --- without sacrificing relevant information content--- is therefore a critical constraint for energy efficient networks of spiking neurons in biology and neuromorphic hardware alike. The inherent complexity of biological neurons provides a possible mechanism to realize a good trade-off between these two conflicting objectives: multi-compartment neuron models can become selective to highly specific input patterns, and thus learn to produce informative yet sparse spiking codes. In this paper, I motivate the operation of a simplistic hierarchical neuron model by analogy to decision trees, show how they can be optimized using a modified version of the greedy decision tree learning rule, and analyze the results for a simple illustrative binary classification problem.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=lThqkZTrFKg&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=27,2021,NICE,Yigit Demirag,ETH Zurich,Dynap-SE1 Demo session,-,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=c3HWeRgvh88&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=28,2021,NICE,Yigit Demirag,ETH Zurich,Simulating Dynap-SE1,-,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=yduGeNjIFnE&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=29,2021,NICE,Charlotte Frenkel,INI Zurich,Bottum-up and top-down neuromorphic processor design,"Abstract—While Moore’s law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of new alternative brain-inspired computing architectures that promise to achieve the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic intelligence represents a paradigm shift in computing based on the implementation of spiking neural network architectures tightly co-locating processing and memory. In this paper, we provide a comprehensive overview of the field, highlighting the different levels of granularity present in existing silicon implementations, comparing approaches that aim at replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down), and assessing the benefits of the different circuit design styles used to achieve these goals. First, we present the analog, mixed-signal and digital circuit design styles, identifying the boundary between processing and memory through time multiplexing, in-memory computation and novel devices. Next, we highlight the key tradeoffs for each of the bottom-up and top-down approaches, survey their silicon implementations, and carry out detailed comparative analyses to extract design guidelines. Finally, we identify both necessary synergies and missing elements required to achieve a competitive advantage for neuromorphic edge computing over conventional machine-learning accelerators, and outline the key elements for a framework toward neuromorphic intelligence.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=X3TOohLp4jk&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=30,2021,NICE,Dylan Paiton,University of Tubingen,Subspace locally competitive algorithms,"We introduce subspace locally competitive algorithms (SLCAs), a family of novel network architectures for modeling latent representations of natural signals with group sparse structure. SLCA first layer neurons are derived from locally competitive algorithms, which produce responses and learn representations that are well matched to both the linear and non-linear properties observed in simple cells in layer 4 of primary visual cortex (area V1). SLCA incorporates a second layer of neurons which produce approximately invariant responses to signal variations that are linear in their corresponding subspaces, such as phase shifts, resembling a hallmark characteristic of complex cells in V1. We provide a practical analysis of training parameter settings, explore the features and invariances learned, and finally compare the model to single-layer sparse coding and to independent subspace analysis.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=V3yib-9fZn8&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=31,2021,NICE,William Kay,ORNL,Graph algorithms in neuromorphic computing,"Neuromorphic computing is poised to become a promising computing paradigm in the post Moore’s law era due to its extremely low power usage and inherent parallelism. Traditionally speaking, a majority of the use cases for neuromorphic systems have been in the field of machine learning. In order to expand their usability, it is imperative that neuromorphic systems be used for non-machine learning tasks as well. The structural aspects of neuromorphic systems (i.e., neurons and synapses) are similar to those of graphs (i.e., nodes and edges), However, it is not obvious how graph algorithms would translate to their neuromorphic counterparts. In this work, we propose a preprocessing technique that introduces fractional offsets on the synaptic delays of neuromorphic graphs in order to break ties. This technique, in turn, enables two graph algorithms: longest shortest path extraction and minimum spanning trees.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=yc8hXOEFLMI&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=32,2021,NICE,Eric Müller,Heidelberg University,BrainScaleS: Development methodologies and operating systems,"BrainScaleS-1 is a wafer-scale mixed-signal accelerated neuromorphic system targeted for research in the elds of computational neuroscience and beyond-vonNeumann computing. The BrainScaleS Operating System (BrainScaleS OS) is a software stack giving users the possibility to emulate networks described in the high-level network description language PyNN with minimal knowledge of the system. At the same time, expert usage is facilitated by allowing to hook into the system at any depth of the stack. We present operation and development methodologies implemented for the BrainScaleS-1 neuromorphic architecture and walk through the individual components of BrainScaleS OS constituting the software stack for BrainScaleS-1 platform operation.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=hvB_2HbiFHQ&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=33,2021,NICE,David Schaffer,Binghamton University,Evolving spiking neural networks for robust sensory-motor decision tasks of varying difficulty,"While there is considerable enthusiasm for the potential of spiking neural network (SNN) computing, there remains the fundamental issue of designing the topologies and parameters for these networks. We say the topology IS the algorithm. Here, we describe experiments using evolutionary computation (genetic algorithms, GAs) on a simple robotic sensory-motor decision task using a gene driven topology growth algorithm and letting the GA set all the SNN's parameters. We highlight lessons learned from early experiments where evolution failed to produce designs beyond what we called ""cheap-tricksters"". These were simple topologies implementing decision strategies that could not satisfactorily solve tasks beyond the simplest, but were nonetheless able to outcompete more complex designs in the course of evolution. The solution involved alterations to the fitness function so as to reduce the inherent noise in the assessment of performance, adding gene driven control of the symmetry of the topology, and improving the robot sensors to provide more detailed information about its environment. We show how some subtle variations in the topology and parameters can affect behaviors. We discuss an approach to gradually increasing the complexity of the task that can induce evolution to discover more complex designs. We conjecture that this type of approach will be important as a way to discover cognitive design principles.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=FSIoGz4MSEo&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=34,2021,NICE,Tej Pandit,UTSA,Relational neurogenesis: For lifelong learning agents,"Reinforcement learning systems have shown tremendous potential in being able to model meritorious behavior in virtual agents and robots. The ability to learn through continuous reinforcement and interaction with an environment negates the requirement of painstakingly curated datasets and hand crafted features. However, the ability to learn multiple tasks in a sequential manner, referred to as lifelong or continual learning, remains unresolved. Current implementations either concentrate on preserving information in fixed capacity networks, or propose incrementally growing networks which randomly search through an unconstrained solution space. This work proposes a novel algorithm for continual learning using neurogenesis in reinforcement learning agents. It builds upon existing neuroevolutionary techniques, and incorporates several new mechanisms for limiting the memory resources while expanding neural network learning capacity. The algorithm is tested on a custom set of sequential virtual environments which emulate meaningful and relevant scenarios.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=mAD_0zEsPCM&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=35,2021,NICE,Julian Goeltz,Heidelberg University,Fast and deep: energy-efficient neuromorphic learning with first-spike times,"For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems also strive for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. In the time-to-first-spike coding framework, both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of error-backpropagation-based learning for hierarchical networks of leaky integrate-and-fire neurons. This narrows the gap between previous existing models of first-spike-time learning and biological neuronal dynamics, thereby also enabling fast and energy-efficient inference on analog neuromorphic devices that inherit these dynamics from their biological archetypes.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=-UPOz7ywdPM&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=36,2021,NICE,Irina Rish,MILA,Beyond Backprop: Different approaches to credit assingment in neural nets,"Backpropagation algorithm (backprop) has been the workhorse of neural net learning for several decades, and its practical effectiveness is demonstrated by recent successes of deep learning in a wide range of applications. This approach uses chain rule differentiation to compute gradients in state-of-the-art learning algorithms such as stochastic gradient descent (SGD) and its variations. However, backprop has several drawbacks as well, including the vanishing and exploding gradients issue, inability to handle non-differentiable nonlinearities and to parallelize weight-updates across layers, and biological implausibility. These limitations continue to motivate exploration of alternative training algorithms, including several recently proposed auxiliary-variable methods which break the complex nested objective function into local subproblems. However, those techniques are mainly offline (batch), which limits their applicability to extremely large datasets, as well as to online, continual or reinforcement learning. The main contribution of our work is a novel online (stochastic/mini-batch) alternating minimization (AM) approach for training deep neural networks, together with the first theoretical convergence guarantees for AM in stochastic settings and promising empirical results on a variety of architectures and datasets.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=saJuQAwXPpQ&list=PLJ506hQ4g3TgOgbEDhZhbkPcTMdtNHmy-&index=37,2021,NICE,Konstantinos Michmizos,Rutgers University,Bioinspired robotic navigation: Real-time mapping on a neuromorphic processor,"Mapping is a critical component for developing a simultaneous localization and mapping (SLAM) system in mobile robots. We draw from the brain's dedicated network that solves the spatial navigation problem by learning a cognitive map of the surrounding environment using networks of specialized neurons, such as place cells, grid cells, head direction cells, and border cells. We further integrated our neuro-inspired network into a neuromorphic processor, namely Intel's Loihi chip. Here, we proposed an SNN that used Winner-Take-ALL (WTA) structure and heterosynaptic competitive learning for place field generation and dendritic trees for reference frame transformation. The network learned distributed sub-maps on place cells, that, when combined, they encode accurately a unified map of the environment. By using an efficient interaction framework between the Robot Operating System (ROS) and Loihi, we showcase how our SNN may run in real-time interacting with a mobile robot equipped with a 360-degree LiDAR sensor. These results pave the way for an efficient neuromorphic SLAM solution on Loihi for robots operating in unknown environments.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=t90wD_GlcZ8&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=3,2019,NICE,Kris Carlson,BrainChip,Akida: A low-power neuromorphic SoC for event-based computation,"Chris Carlson, a research scientist at BrainChip, a small startup company, is discussing their efforts to build low-power neuromorphic systems on a chip for event-based computation. He mentions that their goal is to use a single low-power hardware platform to run conventional deep neural network (DNN) inference algorithms, as well as native spiking neural networks and a subset of event-based algorithms. They also aim to include on-chip unsupervised learning algorithms to enable the system to adapt and learn. The entire network or algorithm should run on the chip, rather than just being passed data from a host computer.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=78JKy5drKXo&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=4,2019,NICE,Mukesh Khare,IBM,The future of AI computing,"The speaker, Mukesh Khare, is the Vice President for Semiconductor and AI Hardware Strategy at IBM Research. He is giving a high-level overview of IBM Research's focus on redefining the future of computing through a combination of three pillars: bits, neurons, and qubits. Bits represent traditional computing and mathematics, neurons represent artificial intelligence and AI hardware, and qubits represent quantum computing. He also welcomes attendees to the Albany site, which he describes as one of the most advanced sites for semiconductor research in a collaborative manner, with many partners from the semiconductor industry ecosystem. He encourages attendees to take tours of the site to see the capability and potential for collaboration.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=pFfJVWAwwJI&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=5,2019,NICE,Steve Furber,University of Manchester,SpiNNaker Applications,"The speaker is Steve Furber, a professor at the University of Manchester. He is discussing the Spinnaker system, a project that began 20 years ago with the goal of using a million mobile phone processors to support real-time biological spiking neural network models, with the aim of contributing to understanding more about how the brain works. The major innovation in Spinnaker is how the processors are connected in real-time to distribute models across a physically large machine. The project started construction in 2005, with the design of the core chip taking 40 man-years of effort and five years of elapsed time. The chip is packaged with an industry standard DRAM for bulk memory. The goal was to build the machine on a budget of about a pound per processor.",ChatGPT,Neuromorphic circuits,Large scale neuromorphic computing platform,,,,,,
https://www.youtube.com/watch?v=u7mNlxCNgDI&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=6,2019,NICE,Humza Syed,Rochester Institute of Technology,Analysis of wide and deep echo state networks for multiscale spatiotemporal time series forecasting,"Echo state networks are computationally lightweight reservoir models inspired by the random projections observed in cortical circuitry. As interest in reservoir computing has grown, networks have become deeper and more intricate. While these networks are increasingly applied to nontrivial forecasting tasks, there is a need for comprehensive performance analysis of deep reservoirs. In this work, we study the influence of partitioning neurons given a budget and the effect of parallel reservoir pathways across different datasets exhibiting multi-scale and nonlinear dynamics.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=o5WeiRte6VI&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=8,2019,NICE,Giacomo Indiveri,INI Zurich,DYNAP=SEL: An ultra-low power mixed signal synamic neuromorphic asynchronous processor with self learning abilities,"The speaker is discussing the field of neuromorphic computing, which involves building dedicated hardware, such as spiking processors, to emulate neural functions and understand how the brain works. He gives credit to the people who have contributed to this field of research at the University of Zurich and explains that their work is driven by basic research, rather than commercial or practical applications. The speaker also mentions that the term ""neuromorphic"" was coined by Carver Mead in the late 1980s, who pioneered this field with his group of students at Caltech. He emphasizes that their research is highly interdisciplinary and requires expertise in neuroscience, computer science, nonlinear dynamical systems, device physics, microelectronics, and material science.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=N-im2G8yk0s&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=9,2019,NICE,Kwabena Boahen,Stanford,Braindrop: A mixed-signal neuromorphic system that presents clean abstractions,"Braindrop is the first mixed-signal neuromorphic system designed to be programmed at a high level of abstraction. Previous neuromorphic systems were programmed at the neurosynaptic level and required expert knowledge of the hardware to use. In stark contrast, Braindrop’s computations are specified as coupled nonlinear dynamical systems and synthesized to the hardware by an automated procedure. This procedure not only leverages Braindrop’s fabric of subthreshold analog circuits as dynamic computational primitives, it compensates for their mismatched and temperature-sensitive responses at the network level. Thus, a clean abstraction is presented to the user. Fabricated in a 28-nm FDSOI process, Braindrop integrates 4096 neurons in 0.65 sq-mm. Two innovations—sparse-encoding through analog spatial convolution and weighted spike-rate summation though digital accumulative thinning—cut digital traffic drastically, reducing the energy Braindrop consumes per equivalent synaptic operation to 380 fJ for typical network configurations.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=U5XrQZIPF3k&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=10,2019,NICE,Cory Merkel,Rochester Institute of Technology,Current mode memristor crossbars for neuromorphic computing,"Motivated by advantages of current-mode design, this paper explores the implementation of weight matrices in neuromemristive systems via current-mode memristor crossbar circuits. After deriving theoretical results for the range and distribution of weights in the current-mode design, it is shown that any weight matrix based on voltage-mode crossbars can be mapped to a current-mode crossbar if the voltage-mode weights are carefully bounded. Then, a modified gradient descent rule is derived for the current-mode design that can be used to perform backpropagation training. Behavioral simulations on the MNIST dataset indicate that both voltage and current-mode designs are able to achieve similar accuracy and have similar defect tolerance. However, analysis of trained weight distributions reveals that current-mode and voltage-mode designs may use different feature representations.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=RPHs-91LqhA&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=11,2019,NICE,Jeff Shainline,NIST Boulder,Fluxonic processing of photonic synapse events,"Much of the information processing performed by a biological neuron occurs in the dendritic tree. For artificial neural systems using light for communication, it is advantageous to convert signals to the electronic domain at synaptic terminals, so dendritic computation can be performed with electrical circuits. Here, we present circuits based on Josephson junctions and mutual inductors that act as dendrites, processing signals from synapses receiving single-photon communication events with superconducting detectors. We show simulations of circuits performing basic temporal filtering, logical operations, and nonlinear transfer functions. We further show how the synaptic signal from a single photon can fan out locally in the electronic domain to enable the dendrites of the receiving neuron to process a photonic synapse event or pulse train in multiple different ways simultaneously. Such a technique makes efficient use of photons, energy, space, and information.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=0UFGNLm9wkk&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=12,2019,NICE,Johannes Schemmel,Heidelberg University,Turing or non-turing? This is the Question: BrainScales 2,"The speaker is discussing the field of ""neuromorphic computing,"" which involves creating artificial intelligence systems that are inspired by biology and neuroscience. They explain that the goal is to create systems that are more energy-efficient and scalable than current AI systems, but that this is a difficult task due to the need for local learning rules and the lack of availability of novel devices for this type of research. They mention that at the moment, the best option is to use CMOS-based bio-inspired neuromorphic systems, but that this is expensive for research groups, and they are not able to show large brain-size systems due to funding limitations.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=7a0pV5y0XCk&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=14,2019,NICE,Mike Davies,Intel,Advancing neuromorphic computing from promise to competitive technology,"The speaker, Mike Davies, is discussing the field of competitive computer architectures and the relationship between deep learning and brain-inspired computation. He presents a three-circle Venn diagram to illustrate the intersection of these three areas. He explains that at Intel, they are in the business of building programmable chips that can be used for a wide range of problems, and that deep learning is a major driver of the rise of intelligent computing. He also notes that machine learning is a source of inspiration for new chip architectures, but that it relies on a traditional von Neumann programming model and on human inspiration. In contrast, brain-inspired computation offers a model of something that works in nature and is incredibly energy efficient and fast. He also mentions that the artificial neural network model used in deep learning began as being neuroscience-inspired but has only become useful and powerful due to the backpropagation algorithm, which allows for offline training of the network.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=7X_OVMogIVU&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=15,2019,NICE,Dimitri Nikonov,Intel,Methodology for benchmarking of neural computing integrated circuits,"The speaker is discussing a methodology for benchmarking beyond CMOS devices, which are being considered as components for neural networks and neuromorphic chips. They are focusing on developing a consistent methodology for benchmarking the boolean digital logic and extending it to neural computing. They are using the perceptron, a basic element of neural networks, as the building block for circuits and are looking at various types of hardware, such as nano magnetic devices, to implement neurons and synapses. They are also looking at different types of resistive elements and other devices to implement synapses, and are using theoretical estimates to determine how these devices will perform in terms of energy, delay, and area. They also mention that their benchmarking is a theoretical framework and not actual experimental benchmarking of running algorithms on hardware.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=ePu1u5EOrj8&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=16,2019,NICE,Peter Blouw,ABR Inc.,Benchmarking keyword spotting on neuromorphic hardware,"Using Intel's Loihi neuromorphic research chip and ABR's Nengo Deep Learning toolkit, we analyze the inference speed, dynamic power consumption, and energy cost per inference of a two-layer neural network keyword spotter trained to recognize a single phrase. We perform comparative analyses of this keyword spotter running on more conventional hardware devices including a CPU, a GPU, Nvidia's Jetson TX1, and the Movidius Neural Compute Stick. Our results indicate that for this inference application, Loihi outperforms all of these alternatives on an energy cost per inference basis while maintaining equivalent inference accuracy. Furthermore, an analysis of tradeoffs between network size, inference speed, and energy cost indicates that Loihi's comparative advantage over other low-power computing devices improves for larger networks.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=CE6_MNCDBLs&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=17,2019,NICE,Campbell Scott,IBM,"Synaptic plasticity in an artificial hebbian network exhibiting continuous, unsupervised, rapid learning","CAL (Context Aware Learning) is an artificial Hebbian network that simulates neurological structure and processes. Hebb's rules for synapse creation and weight update permit CAL to learn rapidly, from few examples, continuously and without supervision. This paper describes recent, neurologically inspired, modifications to the algorithms that lead to faster learning and to greater accuracy of prediction. The importance of plasticity, both structural and in synaptic weights, is illustrated. A mechanism to limit the plasticity of the most relevant synapses leads to minimal forgetting of previously learned sequences.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=mccjWgGZaH4&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=18,2019,NICE,Mihai Petrovici,University of Bern,Computing with physics: From biological to artificial intellegence and back again,"The speaker, Mihai Petrovici, is discussing neuromorphic systems, which are systems that emulate the structure and function of biological neurons. He addresses questions about why these systems are being built, what benefits they hope to gain, and what the potential ""killer application"" of these systems could be. He argues that there is a computational advantage to using spikes in these systems and that the use of spikes is a natural way to make inferences about incomplete and noisy information. The talk seems to focus on the theory and modeling of neuromorphic systems.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=YpkTCfphnMw&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=19,2019,NICE,Felix Wang,Sandia National Laboratories,Spatio-temporal signals processing in polychronizing spiking neural networks,"The ability of an intelligent agent to process complex signals such as those found in audio or video depends heavily on the nature of the internal representation of the relevant information. This work explores the mechanisms underlying this process by investigating theories inspired by the function of the neocortex. In particular, we focus on the phenomenon of polychronization, which describes the self-organization in a spiking neural network resulting from the interplay between network structure, driven spiking activity, and synaptic plasticity. What emerges are groups of neurons that exhibit reproducible, time-locked patterns of spiking activity. We propose that this representation is well suited to spatio-temporal signal processing, as it naturally resembles patterns found in real-world signals. We explore the computational properties of this approach and demonstrate the ability of a simple polychronizing network to learn different spatio-temporal signals.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=rKcm-X8ZoL8&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=20,2019,NICE,Pau Aceituno,Max Planck Institute for Mathematics in the Science,The structure of complex neural networks and its effect on learning,-,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=uaIfPk-Vw7Q&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=21,2019,NICE,Edward Kim,Villanova University,Spatiotemporal sequence memory for prediction using deep sparse coding,"Our brains are, ""prediction machines"", where we are continuously comparing our surroundings with predictions from internal models generated by our brains. This is demonstrated by observing our basic low level sensory systems and how they predict environmental changes as we move through space and time. Indeed, even at higher cognitive levels, we are able to do prediction. We can predict how the laws of physics affect people, places, and things and even predict the end of someone's sentence. In our work, we sought to create an artificial model that is able to mimic early, low level biological predictive behavior in a computer vision system. Our predictive vision model uses spatiotemporal sequence memories learned from deep sparse coding. This model is implemented using a biologically inspired architecture: one that utilizes sequence memories, lateral inhibition, and top-down feedback in a generative framework. Our model learns the causes of the data in a completely unsupervised manner, by simply observing and learning about the world. Spatiotemporal features are learned by minimizing a reconstruction error convolved over space and time, and can subsequently be used for recognition, classification, and future video prediction. Our experiments show that we are able to accurately predict what will happen in the future; furthermore, we can use our predictions to detect anomalous, unexpected events in both synthetic and real video sequences.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=YFL5L73bMDY&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=22,2019,NICE,Yulia Sandamirskaya,Intel,Attractor dynamics and embodiment of neural computing,"In this talk, the speaker discusses the concept of brain inspired computing, specifically focusing on the question of what brains actually compute. They argue that biological neural systems have evolved primarily to generate movement and navigate the environment, and that in order to do so, they require perception, calibration of internal and external parameters, and control. The speaker suggests that in order to achieve intelligent control, two main components are necessary: a working memory to keep track of sensory input, and decision-making to select among alternatives. They also mention that working memory and decision-making are important not only for spatial attention but also for feature-based attention.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=NAj1KPmII7Q&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=23,2019,NICE,Ayon Borthakur,Cornell University,Signal conditioning for learning in the wild,"The mammalian olfactory system learns rapidly from very few examples, presented in unpredictable online sequences, and then recognizes these learned odors under conditions of substantial interference without exhibiting catastrophic forgetting. We have developed a brain-mimetic algorithm that replicates these properties, provided that sensory inputs adhere to a common statistical structure. However, in natural, unregulated environments, this constraint cannot be assured. We here present a series of signal conditioning steps, inspired by the mammalian olfactory system, that transform diverse sensory inputs into a regularized statistical structure to which the learning network can be tuned. This preprocessing enables a single instantiated network to be applied to widely diverse classification tasks and datasets - here including gas sensor data, remote sensing from spectral characteristics, and multi-label hierarchical identification of wild species - without adjusting network hyperparameters.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=Erer1T4KWjg&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=24,2019,NICE,Tadashi Yamazaki,The University of Electro-Communications,Online reinforcement learning by a spiking neural network model of the basal ganglia,"The speaker is discussing the potential use of neuromorphic chips in reinforcement learning, specifically in the context of the basal ganglia in the brain. They propose that online, on-chip, low-power reinforcement learning is a ""killer application"" for neuromorphic chips as it can only be done by these types of processors. They have implemented a circuit based on the anatomical structure of the basal ganglia using C++ and CUDA, and have used a spike-time-based temporal difference long-term potentiation learning rule. They have also been able to simulate the network dynamics in real-time, which allows for reinforcement learning to occur simultaneously during the behavior. They provide an example of a simple maze task to demonstrate how the reinforcement learning can work in practice.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=0BEUkoGz2Qc&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=25,2019,NICE,Hughes Metras,CEA Leti,3D advanced technologies for neuromorphic architectures,"The speaker is discussing the potential benefits of using 3D technologies in neuromorphic systems, specifically in the context of smart retinas and vision processing. He mentions that they have developed a chip that uses a small number of neurons and synapses to classify handwritten numbers, and the majority of the chip's surface area is taken up by interconnects and interfacing. By stacking layers, they believe they can gain a lot of surface area and improve processing capabilities. He also mentions that they have implemented a 3D neural network processor and have seen an average gain of 20% in terms of critical path, power, and wires.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=y5vAwXTrwgg&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=26,2019,NICE,Andrew Sornborger,Los Alamos National Lab.,"A pulse-gated, neural implementation of the backpropagation algorithm","For some time, it has been thought that backpropagation of errors could not be implemented in biophysiologically realistic neural circuits. This belief was largely due to either 1) the need for symmetric replication of feedback and feedforward weights, 2) the need for differing forms of activation between forward and backward propagating sweeps, and 3) the need for a separate network for error gradient computation and storage, on the one hand, or 4) nonphysiological backpropagation through the forward propagating neurons themselves, on the other. In this paper, we present spiking neuron mechanisms for gating pulses to maintain short-term memories, controlling forward inference and backward error propagation, and coordinating learning of feedback and feedforward weights. These neural mechanisms are synthesized into a new backpropagation algorithm for neuromorphic circuits.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=twwjp28tUAI&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=27,2019,NICE,Konstantinos Michmizos,Rutgers University,Towards critical SNNs: Astrocytes detect chaos in neuronal dynamics,"The speaker, Konstantinos Michmizos, is discussing a novel approach to exploring and exploiting the computational role of non-neuronal cells in the brain. He argues that the brain has been computing long before scientists began recording the electrical activity of neurons, and that recent advances in imaging technology have revealed the presence of non-neuronal cells that are involved in processing and learning information in the brain. Michmizos' research focuses on understanding these non-neuronal mechanisms and translating this understanding into ""brain-morphic"" computing. He also discusses a specific project in which they are building robotic controllers based on spiking neural networks, and the potential benefits of this approach in terms of accuracy and energy efficiency.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=cFSM7a7RH7s&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=28,2019,NICE,Brad Aimone,Sandia National Laboratories,Mosaics,"The speaker is discussing the use of neuromorphic hardware and algorithms for solving complex problems. He argues that neural algorithms will have an advantage over conventional approaches when applied to large-scale problems such as deep learning. He also mentions that for smaller problems, conventional approaches such as CPU's may be more efficient. He also mentions that the field of neural algorithms is still in its infancy and that the ultimate goal is to create algorithms that model the brain itself and are useful in the real world. He also states that neuromorphic hardware will be more efficient once the fixed cost of the hardware is overcome and the problem is large enough.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=vuvHmg2gfg4&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=31,2019,NICE,Irina Rish,IBM,AI and neuroscience: Bridging the gap,"The speaker, Irina Rish, is discussing the intersection of AI and neuroscience within IBM Watson, specifically at the Yorktown lab. She mentions that the relationship between the two fields is bi-directional, with AI being applied to brain imaging data and neuroscience inspiring AI methods. The goal is to understand the nature and laws governing intelligence, whether it's artificial or natural. She also mentions that the focus of using AI in neuroscience is on extracting statistical biomarkers, which are features that can be predictive about particular disorders. The speaker also briefly describes a decade of work at the Yorktown lab, where they have been trying to bring more machine learning into neuroimaging and feature extraction.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=C_Dq26AjIew&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=32,2019,NICE,Tony Chiang,Applied Materials,Material engineering for AI compute,"The speaker, Tony Chiang from Applied Materials, is discussing how their company is approaching the field of AI by focusing on materials engineering. They believe that there is potential for new interconnectivity and partnerships between materials and systems in the AI era, and that Applied Materials can play a role in linking the materials innovation to potential AI performance. He also mentions that the company has been in business for 50 years, providing systems and tools for manufacturing in the semiconductor and display industries, and that they pride themselves on innovation. Additionally, he notes that the company has been keeping pace with Moore's Law by developing integrated materials platforms to assist with traditional CMOS scaling, but that as the field of AI progresses, there is a need for a faster connection between materials innovation and potential AI performance.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=D5W0YkNCo_M&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=33,2019,NICE,Stefano Ambrogio,IBM,Towards a chip architecture for acceleration of deep neural networks using analog memory,"The speaker is discussing the use of analog memory for deep learning, specifically the approach being taken by IBM. They argue that analog memory can be good for deep learning, and present a new weight structure that can achieve software equivalent training accuracies on deep neural networks. They also discuss circuit design considerations for making the process faster. They mention that artificial intelligence is a collection of different algorithms and schemes to solve real world problems and that deep learning falls under the category of machine learning, which is not very brain-inspired. They also discuss that there has been a lot of development in AI in recent years due to the availability of algorithms, large amounts of labeled data, and computational power. They also mention that there are two categories of machine learning, one that performs forward inferences, and one that performs training, which is currently done using CPUs and GPUs. They also mention that the most expensive operation in deep neural networks is the multiply and accumulate operation, which is currently performed using GPUs. They also mention different types of analog memory such as RRAM, phase-change memory, magnetic devices and ferroelectric memory.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=W0Wp2i4r4xQ&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=34,2019,NICE,Ryan Dellana,Sandia National Laboratories,Low-power deep learning inference using SpiNNaker neuromorphic platform,"In this presentation we will discuss recent results on using the SpiNNaker neuromorphic platform (48-chip model) for deep learning neural network inference. We use the Sandia Labs developed Whetstone spiking deep learning library to train deep multi-layer perceptrons and convolutional neural networks suitable for the spiking substrate on the neural hardware architecture. By using the massively parallel nature of SpiNNaker, we are able to achieve, under certain network topologies, substantial network tiling and consequentially impressive inference throughput. Such high-throughput systems may have eventual application in remote sensing applications where large images need to be chipped, scanned, and processed quickly. Additionally, we explore complex topologies that push the limits of the SpiNNaker routing hardware and investigate how that impacts mapping software-implemented networks to on-hardware instantiations.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=6j6jwcqBGHU&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=35,2019,NICE,Ryad Benosman,University of Pittsburgh,Neuromorphic general purpose computation using precise timing,"The speaker is discussing the topic of general-purpose computation using precise timing. He explains that he has a background in building sensors, specifically cameras that can see in all directions, and that in his past research he has come to understand that the problem with using images for computation is that they are distorted and non-linear. He argues that the true source of information comes from the brain and the eye, which sample on the amplitude axis instead of the time axis, allowing for an adaptive frequency of information. He also mentions that he has spent many years working on this topic and that his research has led to the development of a type of camera called the DVS.",ChatGPT,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=44XyVsRFz4U&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=36,2019,NICE,Luke Boudreau,Rochester Institute of Technology,Binding the sparse distributed representations in hierarchical temporal memory,"Hierarchical Temporal Memory is a brain inspired theory of intelligence, which emulates the homogeneous structure and connectivity of the pyramidal neurons in the mammalian neocortex. Similar to the neocortex, Hierarchical Temporal Memory processes spatiotemporal information to produce an internal representation of objects in the world. A fundamental component in Hierarchical Temporal Memory is the spatial pooler, which is responsible for processing feedforward sensory inputs into sparse distributed representations. In this work, we apply an explicit binary binding vector operation from vector symbolic architectures to HTM's sparse distributed representations through local inhibition. We demonstrate that the binding operation can be used as the basis for a content addressable memory, which has the potential for storage and retrieval of sparse distributed representations. Furthermore, the bound vectors and their semantic relationships are visualized through dimensionality reduction techniques and a uniqueness matrix.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=xIUvV7_f9f0&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=37,2019,NICE,Yijing Watkins,Los Alamos National Lab.,Unsupervised dictionary learning for neuromorphic processors,"A new class of neuromorphic processors promises to provide fast and power-efficient execution of spiking neural networks with on-chip synaptic plasticity. This efficiency derives in part from the fine-grained parallelism as well as event-driven communication mediated by spatially and temporally sparse spike messages. Another source of efficiency arises from the close spatial proximity between synapses and the sites where their weights are applied and updated. This proximity of compute and memory elements drastically reduces expensive data movements but imposes the constraint that only local operations can be efficiently performed, similar to constraints present in biological neural circuits. Efficient weight update operations should therefore only depend on information available locally at each synapse as non-local operations that involve copying, taking a transpose, or normalizing an entire weight matrix are not efficiently supported by present neuromorphic architectures. Moreover, spikes are typically non-negative events, which imposes additional constraints on how local weight update operations can be performed. The Locally Competitive Algorithm (LCA) is a dynamical sparse solver that uses only local computations between non-spiking leaky integrator neurons, allowing for massively parallel implementations on compatible neuromorphic architectures such as Intel's Loihi research chip. It has been previously demonstrated that non-spiking LCA can be used to learn dictionaries of convolutional kernels in an unsupervised manner from raw, unlabeled input, although only by employing non-local computation and signed non-spiking outputs. Here, we show how unsupervised dictionary learning with spiking LCA (S-LCA) can be implemented using only local computation and unsigned spike events, providing a promising strategy for constructing self-organizing neuromorphic chips.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=YbA6LJtFAu8&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=38,2019,NICE,Constantine Dovrolis,Georgia Tech.,A neuro-inspired computing module for unsupervised continual learning,"We propose that the Continual Learning desiderata can be achieved through a neuro-inspired architecture, grounded on Mountcastle's cortical column hypothesis. The proposed architecture involves a single module, called Self-Taught Associative Memory (STAM), which models the function of a cortical column. STAMs are repeated in multi-level hierarchies involving feedforward, lateral and feedback connections. STAM networks learn in an unsupervised manner, based on a combination of online clustering and hierarchical predictive coding. This short paper only presents the architecture and its connections with neuroscience. A mathematical formulation and experimental results will be presented in an extended version of this paper.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=phmL9hY9GRg&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=39,2019,NICE,Catherine Schuman,ORNL,Shortest path and neighborhood subgraph extraction on a spiking memristive neuromorphic implementation,"Spiking neuromorphic computers (SNCs) are promising as a post Moore’s law technology partly because of their potential for very low power computation. SNCs have primarily been demonstrated on machine learning and neural network applications, but they can also be used for applications beyond machine learning that can leverage SNC properties such as massively parallel computation and collocated processing and memory. Here, we demonstrate two graph problems (shortest path and neighborhood subgraph extraction) that can be solved using SNCs. We discuss the approach for mapping these applications to an SNC. We also estimate the performance of a memristive SNC for these applications on three real-world graphs.",Paper,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=6_qyssEtFMs&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=40,2019,NICE,Greg Cohen,Western Sydney Univ.,Active sensing and its application to neuromorphic space imaging,,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=Vx8zmSBz-g4&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=41,2019,NICE,Michael teti,Florida Atlantic University,"Half the measurements, twice the speed",,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=ac5UhQH8pfM&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=42,2019,NICE,Yakopcic Demange,University of Dayton,Cognitive agents for autonomous robots,,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=4vt6ZLG_X3Q&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=43,2019,NICE,Ed Lawson,"Naval Research Laboratory, Washington DC",Neuromorphic robot perception for autonomous control,,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=BqTREFUa9YY&list=PLJ506hQ4g3Th3sDNaHiqmK6Pr0YTH_aZo&index=44,2019,NICE,Rolf Muller,Virginia Tech.,Neuromorphic computation for autonomous mobility in natural environments,,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=WWsuNTV0mNI,2019,NICE,Andrew Rowley,University of Manchester,Spinnaker Overview,,,Uncategorized,Uncategorized,,,,,,
https://www.youtube.com/watch?v=jFfCiXuRyBE,2019,NICE,Andrew Rowley,University of Manchester,Spinnaker tutorial,,,Uncategorized,Uncategorized,,,,,,
