link,Year,About,Event,Speaker,Organization,Category,Source of Summary,Summary,Topic 1,Sub-topic 1-1,Sub-topic 1-2,Sub-topic 1-3,Topic 2,Sub-topic 2-1,Sub-topic 2-2,Sub-topic 2-3
https://www.youtube.com/watch?v=SU_Fp4xS_g4&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx,2019,Event-based Cameras: Challenges and Opportunities,CVPR Workshop on Event-based Vision,Davide Scaramuzza,INI Zurich,Sensing,ChatGPT,"The speaker is discussing the challenges and opportunities of event-based cameras. They mention that event-based cameras are different from standard cameras because they do not output frames like standard cameras, but instead output a stream of events with unique timestamps that correspond to changes in the image (positive or negative). They also mention that event-based cameras can offer advantages such as high dynamic range, low motion blur, and low power consumption. However, there are also challenges to be addressed, such as processing asynchronous streams and developing algorithms that can effectively utilize the unique characteristics of event-based cameras. The speaker also mentions some potential applications for event-based cameras, such as real-time 3D reconstruction and robotics.",,,,,,,,
https://www.youtube.com/watch?v=zGMzQ34Ksok&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=2,2019,Novel Hardware for Spatial AI,CVPR Workshop on Event-based Vision,Andrew Davison,Imperial College London,Robotics applications,ChatGPT,"The speaker, Andrew Davison, is discussing his work in the field of SLAM (Simultaneous Localization and Mapping), which is the problem of how devices understand and build a representation of the world around them, understand their position within it, and use that information to navigate or interact. He notes that there are now a number of products on the market that use SLAM in some form, such as robot vacuum cleaners and drones. He argues that the field of SLAM is evolving into something bigger, which he calls ""spatial AI,"" and that this field has the potential to lead to game-changing products such as general-purpose home robotics and mass-market augmented reality devices. He also acknowledges that there are significant challenges to overcome in order to achieve these goals.",,,,,,,,
https://www.youtube.com/watch?v=7fAPckjQSGE&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=3,2019,Industrial DVS Design: Key Features and Applications,CVPR Workshop on Event-based Vision,Hyunsurk Eric Ryu.,Samsung,Sensing,ChatGPT,"The speaker in the video is a representative from Samsung Electronics, and they are discussing the development and applications of DVS (dynamic vision sensor) chips. The company started developing these chips in 2012 and has made several versions since then. The main focus of the development has been to improve the chip's specialization, data bandwidth, and power consumption. The speaker also mentions that one of the main challenges in designing DVS chips is that the data generation from the sensor depends on the contrast of the object being captured and the illumination condition. The company has developed a technology to increase the bandwidth and video quality of the chips, which helps to mitigate some of these challenges.",,,,,,,,
https://www.youtube.com/watch?v=aDzFSG4yV0M&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=4,2019,Unsupervised Learning of Optical Flow and Camera Motion from Event Data,CVPR Workshop on Event-based Vision,Alex Z. Zhu,University of Pennsylvania,Learning,ChatGPT,"The video is about unsupervised learning methods for event cameras at the University of Pennsylvania. The speaker, Alex Z. Zhu, explains the motivation for using event cameras, which include their ability to capture high-speed motion and high dynamic range without the need for high frame rates. He also mentions that the latency and dynamic range of event cameras are coupled and that they are good for a wide range of motion speeds, not just very fast motions. The speaker then discusses their initial research, which involved designing algorithms that resemble traditional computer vision techniques such as feature tracking for events using an expectation maximization algorithm to solve for optical flow.",,,,,,,,
https://www.youtube.com/watch?v=jhQgElvtb1s&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=5,2019,Realizing the Promise of Spiking Neuromorphic Hardware,CVPR Workshop on Event-based Vision,Mike Davies,Intel,Generic introduction,ChatGPT,"The speaker, Mike Davies, is discussing the potential of neuromorphic computing, specifically spike-based computing, for solving real-world problems. He explains that the traditional von Neumann model of computing may not be sufficient for certain intelligent workloads and that new architectures are needed to achieve more efficient computing. The spike-based neural network model is seen as a simple enhancement to the traditional model that greatly broadens the algorithmic space of what neural networks are capable of computing. The speaker also mentions a chip developed by Intel called Loihi, named after an underwater volcano, which is designed to implement spiking neural networks.",,,,,,,,
https://www.youtube.com/watch?v=wCs2lv3g4A4&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=6,2019,Spiking Neural Networks for Event-based Vision,CVPR Workshop on Event-based Vision,Garrick Orchard,Intel,Neuromorphic-compatible algorithms,ChatGPT,"The speaker is discussing recent work on using spiking neural networks for event-based vision. They are focusing on the use of these networks to detect basic features, such as the orientation of a line, from an event stream. The speaker describes how a particular neuron can act as a coincidence detector, detecting the spatial pattern of activation across multiple pixels connected to it via synapses. They also mention previous work on the H max model, which is a frame-based algorithm for image recognition, and how they are attempting to port that to the spike domain using Gabor filters and pooling layers.",,,,,,,,
https://www.youtube.com/watch?v=_2NWHyM_Bds&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=7,2019,Object Motion Estimation and Grouping from Event Data,CVPR Workshop on Event-based Vision,Cornelia Fermüller,University of Maryland,Neuromorphic-compatible algorithms,ChatGPT,"The video is discussing motion segmentation, which is an important problem in event-based processing for DVS (dynamic vision sensor) cameras. The speaker, Cornelia Fermüller, and her collaborators have been working on independent motion segmentation for fast and challenging scenes. They use event data, which is generated by DVS cameras and is inspired by the mammalian transient pathway, to improve motion segmentation. Fermüller also describes various stimuli that demonstrate how the event data can be used for motion segmentation. The video also mentions that they have used optimization approaches, machine learning techniques, and implemented the techniques in a vector frameworks to make the process faster.",,,,,,,,
https://www.youtube.com/watch?v=D3VcmkQiPR4&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=8,2019,SCAMP-5: Vision Sensor with Pixel Parallel SIMD Processor Array,CVPR Workshop on Event-based Vision,Piotr Dudek,University of Manchester,Sensing,ChatGPT,"The video is a talk given by Piotr Dudek of the University of Manchester about a vision sensor with a pixel parallel SIMD processor array. The talk discusses the difficulty of designing embedded systems for vision processing, as it requires high performance and low power consumption. Dudek suggests that one solution is to eliminate the conventional image sensor and replace it with a smarter sensor that processes information on the sensor itself and sends meaningful data instead of streams of images. This approach has benefits such as high performance, low latency, and reduced power consumption and size of the system. The talk also mentions that event cameras, which detect changes in intensity, are one example of this approach, but that there may be other types of features that could be extracted from images before transmitting data from the sensor.",,,,,,,,
https://www.youtube.com/watch?v=-ZSQWO_9gyc&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=9,2019,Asynchronous Convolutions and Image Reconstruction,CVPR Workshop on Event-based Vision,Robert Mahony,Australian National University,Sensing,ChatGPT,"The speaker is discussing a method for processing data from event cameras, which are a type of camera that outputs a stream of events rather than traditional image data. The speaker is from a systems and control background and is interested in using event cameras for robotics applications. The approach involves using an FPGA (field-programmable gate array) to handle the event processing, which allows for more complex computations to be done on the rest of the system. The speaker is focused on image reconstruction and gradient computation, and is approaching the problem by treating event streams as continuous time systems. They explain that direct integration, which is a method of rebuilding an image from an event stream by summing up all the events that have occurred in that particular image, is one approach that has been used in the past.",,,,,,,,
https://www.youtube.com/watch?v=JcgboJ_7JAE&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=10,2019,Bringing a Blurry Frame Alive at High Frame-Rate with an Event Camera,CVPR Workshop on Event-based Vision,Yuchao Dai,Australian National University,Sensing,ChatGPT,"The video presents a research study on how to bring a blurry frame to life and achieve high frame-rate reconstruction using an event camera. The researchers aim to combine the output of intensity images and events from the camera to achieve their goal. They focus on the unique characteristics of event cameras, such as high temporal resolution and the ability to capture dynamic scenes without blur. They propose a solution that involves a single variable optimization problem and use logarithmic integration of event data to reconstruct the current intensity signal. The goal is to produce a high-frame-rate video from a single blurry frame.",,,,,,,,
https://www.youtube.com/watch?v=1LZKtnQ-6lA&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=11,2019,Events-to-Video: Bringing Modern Computer Vision to Event Cameras,CVPR Workshop on Event-based Vision,Henri Rebecq,INI Zurich,Sensing,ChatGPT,"In this video, the presenter discusses a research project called ""Events-to-Video: Bringing Modern Computer Vision to Event Cameras."" The goal of the project is to turn event data from event cameras, which have high temporal resolution and high dynamic range, into high-quality video. The team used a recurrent neural network and a volumetric voxel grid to convert the event data into an image reconstruction. To train the system, they used a synthetic data generated in a 3D virtual world. The presenter shows examples of their system in action, including high frame rate videos and videos with high dynamic range.",,,,,,,,
https://www.youtube.com/watch?v=jHcxQRC-7iQ&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=12,2019,EventNet: Asynchronous recursive event processing,CVPR Workshop on Event-based Vision,Yusuke Sekikawa,Denso IT Lab Inc. Japan,Event-based systems,ChatGPT,"EventNet is a bit-based neural network designed to efficiently process event streams, which are sequences of events that occur at varying rates and lengths. The goal of EventNet is to create an end-to-end trainable neural network that can model the relationships between a set of events and a target output. The network is inspired by PointNet, which was designed to process sets of points directly without identification and is invariant to the permutation of input points. However, PointNet was designed to process set of inputs in batch manner and cannot process sequential data recursively. EventNet overcomes this computational difficulty by processing past event signals recursively, using a lookup table, realization of MLP and to module architecture.",,,,,,,,
https://www.youtube.com/watch?v=PK2qaqHv5aw&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=13,2019,Neuromorphic Computing: towards event-based cognitive sensing and control,CVPR Workshop on Event-based Vision,Yulia Sandamirskaya,Intel,Event-based systems,ChatGPT,"The speaker is discussing neuromorphic computing, which involves using event-based sensors to produce streams of events, and then fitting that data into a computer. They note that the traditional computing architecture of off-the-shelf computers doesn't fit well with the task of processing the massively parallel stream of sensor information that comes from event-based sensors. They suggest that there is a need for hardware that better matches event-based processing, such as neuromorphic devices, which are hardware implementations of neuronal networks, specifically spiking networks. The speaker also mentions that it is possible to take off-the-shelf convolutional neural networks (CNNs) and use a particular framework to transfer them into spiking neural networks (SNNs), but that this will not give the same performance as the original CNN and some tweaking of the parameters is necessary.",continue,,,,,,,
https://www.youtube.com/watch?v=VgCJ8sldWN8&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=14,2019,Bringing computation on the focal plane: algorithms and systems for sensors,CVPR Workshop on Event-based Vision,Julien N.P. Martel,Stanford,Sensing,ChatGPT,"The speaker in the video, Julien N.P. Martel, is discussing the concept of using computation on the focal plane of sensors in order to perform advanced vision algorithms. He explains that the interaction of light and matter is complex and that optics and sensors can capture properties of this interaction and produce a ""visual code"". Vision algorithms then act as a ""decoder"" to extract visual quantities of interest, such as the motion of objects or properties of light sources. He goes on to explain that there are many different types of sensors that can be used to measure various visual quantities, such as light intensity or polarization. He also notes that by understanding the relationships between different visual quantities, it is possible to infer information about a scene even if a specific sensor for that information does not exist.",,,,,,,,
https://www.youtube.com/watch?v=4Cv7QEeNpTg&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=15,2019,Learning from Events: on the Future of Machine Learning for Event-based Cameras,CVPR Workshop on Event-based Vision,Amos Sironi,Prophesee,Sensing,ChatGPT,"The speaker, Musa Roni, is leading the machine learning team at Prophesee and in this video, he discusses the company's work and future direction in the field of machine learning for event-based cameras. He explains that as the size of the pixels in Prophesee's sensors have decreased, the resolution of the sensor has increased, allowing for greater detail in the images captured. He also mentions that the company has moved from a custom interface to a standard one, making it easier to integrate with other industrial frameworks and that they have implemented some event signal processing directly in the sensor. He also talks about the company's work in learning from the event data, including optical flow and object detection, and compares their approach to the state of the art in the field.",,,,,,,,
https://www.youtube.com/watch?v=2lsQ3tW7OzU&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=16,2019,"Applications, Software and Hardware for Event-Based Vision",CVPR Workshop on Event-based Vision,Kynan Eng,iniVation,Generic introduction,ChatGPT,"The speaker is discussing event-based vision and the importance of a community of developers to advance the technology. They mention that in 2005, the DVS (dynamic vision sensor) was used outside of the lab for the first time in an end-to-end neuromorphic visual system. They mention that since then, the DVS has been in use at 250 organizations, and that the goal is to make it as open and easy to use as possible, with open-source development environments and pre-built modules to interface with other systems. They also mention a new software solution that has been developed which is written in C++ and is more robust, open API, and cross-platform.",,,,,,,,
https://www.youtube.com/watch?v=9IJwF9xYEoU&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=17,2019,Event-based Vision for Augmented Reality,CVPR Workshop on Event-based Vision,Stefan Isler,Insightness,Sensing,ChatGPT,"This video discusses the concept of event-based vision for augmented reality, where the system is able to respond to specific events in the real world in order to create a more interactive and immersive experience for the user. The speaker mentions that this technology has the potential to spark innovation in a variety of industries, including entertainment and industrial, and that it could open up many new worlds and possibilities. They also mention the idea of a ""social interface"" which could be used to interact with the digital world and the potential for the technology to be used in the suburbs, and compare it with the fictional character Tony Stark. The speaker also mentions the importance of accurate sensing and low latency in this kind of system, and suggests that the technology could be used to create dynamic, interactive objects in the user's field of view.",,,,,,,,
https://www.youtube.com/watch?v=kzUHip3DAd0&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=18,2019,Introduction of Celex Family Sensor and Event/Frame/Optical-flow Hybrid Processing,CVPR Workshop on Event-based Vision,Shoushun Chen,CelePixel,Sensing,ChatGPT,"The video is an introduction to the Celex family sensor developed by the Shanghai-based company CelePixel. The sensor is a spinoff from a university and is designed to have low latency and reduce redundant data, which can increase the response time of a vision system. The sensor also uses a new kind of event packet that provides both spatial and temporal information, which can be used for tracking and object identification or classification. The company has been in operation since 2011, and has developed sensors with resolution up to 1 million pixels, with a focus on the automotive industry.",,,,,,,,
https://www.youtube.com/watch?v=XuYYhyS8IwM&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=19,2019,Star Tracking using an Event Camera,CVPR Workshop on Event-based Vision,Tan-Jun Chin,University of Adelaide,Aerospace applications,Paper,"Star trackers are primarily optical devices that are used to estimate the attitude of a spacecraft by recognising and tracking star patterns. Currently, most star trackers use conventional optical sensors. In this application paper, we propose the usage of event sensors for star tracking. There are potentially two benefits of using event sensors for star tracking: lower power consumption and higher operating speeds. Our main contribution is to formulate an algorithmic pipeline for star tracking from event data that includes novel formulations of rotation averaging and bundle adjustment. In addition, we also release with this paper a dataset for star tracking using event cameras1 . With this work, we introduce the problem of star tracking using event cameras to the computer vision community, whose expertise in SLAM and geometric optimisation can be brought to bear on this commercially important application.",,,,,,,,
https://www.youtube.com/watch?v=Bwmmt7dqTIw&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=20,2019,Real-Time 6DOF Pose Relocalization for Event Cameras with Stacked Spatial LSTM Networks,CVPR Workshop on Event-based Vision,Anh Nguyen,IIT,Sensing,Paper,"We present a new method to relocalize the 6DOF pose of an event camera solely based on the event stream. Our method first creates the event image from a list of events that occurs in a very short time interval, then a Stacked Spatial LSTM Network (SP-LSTM) is used to learn the camera pose. Our SP-LSTM is composed of a CNN to learn deep features from the event images and a stack of LSTM to learn spatial dependencies in the image feature space. We show that the spatial dependency plays an important role in the relocalization task with event images and the SP-LSTM can effectively learn this information. The extensively experimental results on a publicly available dataset show that our approach outperforms recent state-of-the-art methods by a substantial margin, as well as generalizes well in challenging training/testing splits. The source code and trained models are available at https:// github.com/ nqanh/ pose relocalization",,,,,,,,
https://www.youtube.com/watch?v=AuXN7y3bMqo&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=21,2019,EV-SegNet: Semantic Segmentation for Event-based Cameras,CVPR Workshop on Event-based Vision,Inigo Alonso,Universidad Zaragoga,Sensing,Paper,"Event cameras are very promising sensors which have shown several advantages over frame-based cameras. Deep learning based approaches, which are leading the state-ofthe-art in visual recognition tasks, could potentially take advantage of the benefits of these cameras, but some adaptations are still needed in order to effectively work on event data. This work introduces the first baseline for semantic segmentation with this kind of data. We build a semantic segmentation CNN based on state-of-the-art techniques which takes event information as the only input. Besides, we propose a novel representation for DVS data that outperforms previously used event representations for related tasks. Since there is no existing labeled dataset for this task, we propose how to automatically generate approximated semantic segmentation labels for some sequences of the DDD17 dataset, which we publish together with the model, and demonstrate they are valid to train a model for DVS data only. We compare our results on semantic segmentation from DVS data with results using corresponding grayscale images, demonstrating how they are complementary and worth combining.",,,,,,,,
https://www.youtube.com/watch?v=S1OLnbnB_qo&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=22,2019,Learning Event-based Height from Plane and Parallax,CVPR Workshop on Event-based Vision,Kenneth Chaney,University of Pennsylvania,Sensing,Paper,"In this work, we propose a fast method to perform event-based structure estimation for vehicles traveling in a roughly 2D environment (e.g. in an environment with a ground plane). Our method transfers the method of plane and parallax to events, which, given the homography to a ground plane and the pose of the camera, generates a warping of the events which removes the optical flow for events on the ground plane, while inducing flow for events above the ground plane. We then estimate dense flow in this warped space using a self-supervised neural network, which provides the height of all points in the scene. We evaluate our method on the Multi Vehicle Stereo Event Camera dataset, and show its ability to rapidly estimate the scene structure both at high speeds and in low lighting conditions.",,,,,,,,
https://www.youtube.com/watch?v=JKRSeg3WrGw&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=23,2019,Asynchronous Convolutional Networks for Object Detection in Neuromorphic Cameras,CVPR Workshop on Event-based Vision,Marco Cannici,Politecnico di Milano,Neuromorphic-compatible algorithms,Paper,"Event-based cameras, also known as neuromorphic cameras, are bioinspired sensors able to perceive changes in the scene at high frequency with low power consumption. Becoming available only very recently, a limited amount of work addresses object detection on these devices. In this paper we propose two neural networks architectures for object detection: YOLE, which integrates the events into surfaces and uses a frame-based model to process them, and fcYOLE, an asynchronous event-based fully convolutional network which uses a novel and general formalization of the convolutional and max pooling layers to exploit the sparsity of camera events. We evaluate the algorithm with different extensions of publicly available datasets, and on a novel synthetic dataset.",,,,,,,,
https://www.youtube.com/watch?v=nFUAQYk3tYA&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=24,2019,DHP19: Dynamic Vision Sensor 3D Human Pose Dataset,CVPR Workshop on Event-based Vision,Enrico Calabrese,INI Zurich,Software and tools,Paper,"Human pose estimation has dramatically improved thanks to the continuous developments in deep learning. However, marker-free human pose estimation based on standard frame-based cameras is still slow and power hungry for real-time feedback interaction because of the huge number of operations necessary for large Convolutional Neural Network (CNN) inference. Event-based cameras such as the Dynamic Vision Sensor (DVS) quickly output sparse moving-edge information. Their sparse and rapid output is ideal for driving low-latency CNNs, thus potentially allowing real-time interaction for human pose estimators. Although the application of CNNs to standard framebased cameras for human pose estimation is well established, their application to event-based cameras is still under study. This paper proposes a novel benchmark dataset of human body movements, the Dynamic Vision Sensor Human Pose dataset (DHP19). It consists of recordings from 4 synchronized 346x260 pixel DVS cameras, for a set of 33 movements with 17 subjects. DHP19 also includes a 3D pose estimation model that achieves an average 3D pose estimation error of about 8 cm, despite the sparse and reduced input data from the DVS.",,,,,,,,
https://www.youtube.com/watch?v=BfMjtUQwWnQ&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=25,2019,CED: Color Event Camera Dataset,CVPR Workshop on Event-based Vision,Cedric Scheerlinck,INI Zurich,Software and tools,Paper,"Event cameras are novel, bio-inspired visual sensors, whose pixels output asynchronous and independent timestamped spikes at local intensity changes, called ‘events’. Event cameras offer advantages over conventional framebased cameras in terms of latency, high dynamic range (HDR) and temporal resolution. Until recently, event cameras have been limited to outputting events in the intensity channel, however, recent advances have resulted in the development of color event cameras, such as the ColorDAVIS346. In this work, we present and release the first Color Event Camera Dataset (CED), containing 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes, which we hope will help drive forward event-based vision research. We also present an extension of the event camera simulator ESIM [1] that enables simulation of color events. Finally, we present an evaluation of three state-of-the-art image reconstruction methods that can be used to convert the ColorDAVIS346 into a continuous-time, HDR, color video camera to visualise the event stream, and for use in downstream vision applications. Website: http://rpg.ifi.uzh.ch/CED",,,,,,,,
https://www.youtube.com/watch?v=eWBEJOr056E&list=PLeXWz-g2If97iGiuBHmnW8IFIxwvSeCHx&index=26,2019,Event-Based Attention and Tracking on Neuromorphic Hardware,CVPR Workshop on Event-based Vision,Yulia Sandamirskaya,INI Zurich,Neuromorphic platforms,Paper,"We present a fully event-driven vision and processing system for selective attention and tracking, realized on a neuromorphic processor Loihi interfaced to an event-based Dynamic Vision Sensor DAVIS. The attention mechanism is realized as a recurrent spiking neural network that implements attractor-dynamics of dynamic neural fields. We demonstrate capability of the system to create sustained activation that supports object tracking when distractors are present or when the object slows down or stops, reducing the number of generated events.",,,,,,,,
https://www.youtube.com/watch?v=LdfMTDS9rqI&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7,2021,Event Computer Vision 10 years Assessment,CVPR Workshop on Event-based Vision,Ryad Benosman,University of Pittsburgh,Generic introduction,Paper,"Abstract: The field of Event-based vision has started within the neuromorphic community decades ago with the promise to develop a new paradigm for sensing and computation inspired by brain structures. For decades, this endeavor has been an exercise in pure research, but over the past 10 years my lab and other investigators have been pursuing this approach to build practical vision systems available to laymen that could be bought off-the-shelf and used by all. Event-based sensors introduce a radical change in Computer Vision because they sample the visual information asynchronously based on pixels “deciding” when information must be acquired. Although some Event cameras allows the acquisition of frames or absolute illuminance, I will shine light on the generic approach required to process visual events in order to make full use of the low power low latency properties of these sensors. I will explain what the canonical structure of an event-based visual application. I will finally explain based on a deep knowledge of the field and the market where all this is heading and what major achievements are waiting to be made and what is currently preventing them from happening.",,,,,,,,
https://www.youtube.com/watch?v=JyaNT69GWO0&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=2,2021,Event-driven convolution based processing,CVPR Workshop on Event-based Vision,Bernabé Linares-Barranco,IMSE-CNM,Event-based systems,Paper,"Abstract: We review some of the event-driven hardware developments in which our lab has been involved, covering from sensitive-DVS to event-driven convolutions on dedicated ASICs, FPGAs, and the SpiNNaker platform, with applications in object recognition or stereo vision. We show how to train event-driven convnets to minimize the number of required spikes, reducing energy consumption for the same recognition tasks. Additionally, we present some results on a type of spike-timing-dependent-plasticity, which uses only binary weights combined with stochasticity, and which results in hardware that requires less hardware and energy resources for the same accuracy.",,,,,,,,
https://www.youtube.com/watch?v=tiK9idp4aaY&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=3,2021,Neuromorphic Vision Applications: From Robotic Foosball to Tracking Space Junk.,CVPR Workshop on Event-based Vision,Gregory Cohen,Western Sydney Univ.,Generic introduction,Paper,"Abstract: Neuromorphic event-based cameras offer a different way to approach visual imaging tasks and really excel at problems in which they can leverage the unique way that the hardware works. This talk will introduce a range of applications for neuromorphic cameras ranging from tracking space junk and satellites to their applications in robotic foosball and pinball. We will demonstrate real-world results from space tracking with event-based cameras, and introduce our Astrosite mobile neuromorphic telescope observatories - built specifically to leverage the benefits of neuromorphic space imaging. We will describe some of the problems with benchmarking and comparing neuromorphic systems, and show how robotic foosball and robotic pinball machines may be a great way to demonstrate the benefits of neuromorphic systems.",,,,,,,,
https://www.youtube.com/watch?v=E1ScHzUUeXA&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=4,2021,High-Performance Neuromorphic Vision: From Core Technologies to Applications,CVPR Workshop on Event-based Vision,Kynan Eng,iniVation,Generic introduction,Paper,"Abstract: Neuromorphic event-based vision can enable new levels of enhanced vision sensing in situations where current technologies fail. In this presentation, we provide an overview of our technology, our DV open developer environment, and some real-world application examples.",,,,,,,,
https://www.youtube.com/watch?v=CFdwBVawVkM&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=5,2021,From the lab to the real world: event-based vision evolves as a commercial force,CVPR Workshop on Event-based Vision,Luca Verre,Prophesee,Generic introduction,Paper,"Abstract: Neuromorphic-based vision has been a popular topic at conferences and technical discussions for many years. Thousands of man years of research have gone into innovative approaches for applying it to applications that improve safety, convenience, automation and efficiency in many aspects of our lives. Today, we are at the cusp of this truly paradigm-shifting approach becoming a commercial reality. From self-driving vehicles, to smart manufacturing and logistics, to scientific research, even space exploration as we have heard from other talks at this conference – we are at the dawn of a new era that will see broader adoption of this technology. At the heart of this movement is Event-Based Vision. By the very fact that we are all gathered for this outstanding event shows the interest and progress we have made, notably by the number of commercial entities who join me as speakers. It is a testament to the dedication of a generation of engineers and researchers in this field. The next step is building on that investment and developing a true ecosystem around the technology. This will require not just companies like Prophesee who implement the core sensors and software to deliver event-based vision as a usable technology, but also a range of software developers, system integrators, distribution partners, academic researchers, standards bodies, even governments to align behind this movement and facilitate even broader adoption. This presentation will frame that movement against the backdrop of the vast new potential of event-based vision in areas such as industrial automation, security and surveillance, mobile, IoT, health and AR/VR.",,,,,,,,
https://www.youtube.com/watch?v=U0ghh-7kQy8&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=6,2021,Event-based Visual Odometry: A Short Tutorial,CVPR Workshop on Event-based Vision,Yi Zhou,HKUST,Sensing,ChatGPT,"The video is discussing the use of event-based cameras for visual odometry, a technique used to estimate the position and orientation of a camera in 3D space. Event-based cameras are a type of bioinspired sensor that acquire visual information in a different way from traditional cameras, producing a stream of asynchronous per-fix intensity changes instead of intensity frames at a fixed frame rate. The video covers the challenges and potential advantages of using event-based cameras for visual odometry, and introduces recent work on event-based stereo visual odometry. The video also provides a brief literature review on the two sub-problems of event-based visual odometry: camera pose tracking and 3D mapping, and highlights two system pipelines. The video concludes by summarizing the talk and outlining remaining open questions in event-based visual odometry.",,,,,,,,
https://www.youtube.com/watch?v=6ciNkcjV6EI&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=7,2021,High Speed Perception-Action Systems with Event-Based Cameras,CVPR Workshop on Event-based Vision,Anthony Bisulco,Samsung,Robotics applications,Paper,"Abstract: High-speed perception-action systems are important for mobile robot systems to react in dynamic environments. Event-based cameras have attractive properties for these systems such as high dynamic range, efficient energy use and low latency sensing. At Samsung’s AI Center in NY (SAIC-NY) we have been working on novel DVS-based systems and algorithms to capitalize on these properties. Our previous work in this domain includes a near-chip architecture for low-complexity pedestrian detection on bandwidth-limited networks. In this talk, we will present an overview of our most recent work where the goal is to create high speed perception-action systems for collision avoidance. The introduction of robots to kitchen environments will require avoidance of incoming high-speed moving obstacles such as falling spices, liquids or sharp objects that they should avoid. Our experimental test-bed to explore these systems consists of shooting a toy-dart(22m/s) at a target located on a linear-actuator with a static event-based camera observing the motion head-on. During the dart’s flight, we developed a perception system to extract time to collision and impact location on the camera plane from the event-stream for triggering a collision avoidance system. The entire dart flight is around 150ms, hence we also analyze the various latencies of the perception-action system and system tradeoffs for collision avoidance. As a result of this analysis, we found an initial observability latency of the dart up to 100ms, which resulted in the use of a telescopic lens to reduce this delay to 20ms. A benefit of using an event-camera in this scenario as opposed to a 60Hz frame-based imager is that the perception process can acquire ~100ms of in-focus events as opposed to one or two motion blurred frames. Inspecting our perception performance using event-data, we established our perception system to estimate time to collision within 24.73% and impact location within 18.4mm on our testing dataset. Overall, our perception system and minimal system latency allows our system to successfully avoid a fast incoming toy dart.",,,,,,,,
https://www.youtube.com/watch?v=5FBEqOTgxnM&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=8,2021,Development of Event-based Sensor and Applications,CVPR Workshop on Event-based Vision,Shoushun Chen,CelePixel,Sensing,Paper,"Abstract: Event cameras have demonstrated great potential to solve problems in many applications such as robotics, mobile, automotive, gaming and computer vision etc. This talk will introduce the recent development by CelePixel. We will first revisit the pixel architecture, then discuss on the limiting factors of the temporal resolution which could be applicable to other event sensors, finally we will introduce an efficient event-based HCI framework.",,,,,,,,
https://www.youtube.com/watch?v=GSJRszrffJI&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=9,2021,Event-based vision and processing for tiny drones,CVPR Workshop on Event-based Vision,Guido de Croon,TU Delft,Robotics applications,Paper,"Abstract: Event-based vision and processing hold an important promise for creating autonomous tiny drones. Both promise to be light weight and highly energy efficient, while allowing for high-speed perception and control. For tiny drones, these characteristics are essential, as they are extremely restricted in terms of size, weight and power, while at smaller scales drones become even more agile. In my talk, I present our work on developing event-based perception and control for tiny autonomous drones. I delve into the approach we followed for having spiking neural networks learn visual tasks such as optical flow estimation. Furthermore, I explain our ongoing effort to integrate these networks in autonomously flying drones.",,,,,,,,
https://www.youtube.com/watch?v=p5kjt0jsuNg&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=10,2021,Event-Based Computer Vision At Sony Advanced Visual Sensing,CVPR Workshop on Event-based Vision,Christian Brändli,Sony,Event-based systems,Paper,"Abstract: Sony Advanced Visual Sensing is a research center of Sony Semiconductor Solutions, the world leader in image sensors. With a long history in the field, Sony AVS works on event-based vision sensors (EVS) and computer vision algorithms. First, the talk will introduce some core principles of event-based processing which have been gathered over the years. The second part of the talk will then highlight some recent applications of event-based algorithms developed at Sony AVS.",,,,,,,,
https://www.youtube.com/watch?v=H2ofs_3rd0Q&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=11,2021,Routing Events in Two-Dimensional Arrays with a Tree,CVPR Workshop on Event-based Vision,Kwabena Boahen,Stanford,Circuit and chip design,Paper,"Abstract: These days, imagers are built by stacking two chips, one with photodetectors, the other with readout electronics. A through-silicon-via or flip-chip bond-pad delivers each signal from the two-dimensional (2D) array of pixels. This approach abandons feeding pixel signals to interface circuitry on the p periphery with one or two shared wires per row or column. Instead, pixels signals may be fed to the leaves of a tree, whose nodes are distributed throughout the readout chip. We present such a tree router, tailored to 2D arrays of pixels, or of small clusters of silicon neurons; each of its nodes has four daughters. Layout is fractal (H-tree); this uses less wiring per signal than a grid. Signaling is serial; this keeps link-width constant (regardless of payload size). To route from the tree's leaves to its root (or vise versa), each node prepends (consumes) a delay-insensitive 1-of-4 code that signals the route's previous (next) branch; additional codes carry payload (2 bits each). We deploy this serial H-tree router to service a 16×16 array of silicon-neuron clusters, each with 16 spike-generating analog somas, 4 spike-consuming analog synapses, and one 128-bit SRAM. Fabricated in a 28-nm CMOS process, the router communicates 26.8M soma-generated and 18.3M synapse-targeted spikes per second while occupying 43% of the client's 35.1×36.1sq-μm.",,,,,,,,
https://www.youtube.com/watch?v=mzwWI3DjMXI&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=12,2021,Towards Asynchronous SLAM with Event Cameras.,CVPR Workshop on Event-based Vision,Ignacio Alzugaray,ETH Zurich,Robotics applications,Paper,"Abstract: Event cameras present unique characteristics that make them especially suitable for robust robotic perception such as high temporal resolution, high dynamic range and low power consumption. These sensors, however, produce an asynchronous and sparse stream of events, making the direct application of traditional computer vision algorithms originally designed for frame-based cameras unfeasible. In this talk we will present an overview of the works developed at the Vision for Robotics Lab which are directed towards the development of an efficient and event-driven pipeline for visual SLAM that can fully exploit the asynchronicity and sparsity of the event stream.",,,,,,,,
https://www.youtube.com/watch?v=rSc8tvdjlhQ&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=13,2021,Learning Spatiotemporal Filters to Track Visual Saliency.,CVPR Workshop on Event-based Vision,Ralph Etienne-Cummings,Johns Hopkins University,Sensing,Paper,"Abstract: Uncovering the nuances behind visual saliency, or the tendency to gaze in a particular direction or toward a specific object, is critical in understanding what and why the human mind focuses on specific features in a field of vision. There are a wide variety of applications in which saliency would provide significant steps forward, such as: tele-tourism, high-accuracy drone cameras, live-data analysis for traffic, and criminal investigations. More specifically, visual saliency in the form of event-based information is particularly attractive because event-based data encodes information in a more compressed and power efficient manner. In this workshop, we discuss an unsupervised learning scheme to learn spatiotemporal filters that can identify and track salient features in an event-based data stream. We show how decision trees and threshold tracking can learn interesting features that are not easily discernable by the human eye, and further compare our findings to a ground-truth human-based saliency experiment with event-based data. We compare hand-crafted versus learned filters with that of the ground-truth human-based data and stress the need for the first event-based visual saliency ground-truth dataset.",,,,,,,,
https://www.youtube.com/watch?v=0mO55e-5_qs&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=14,2021,Hardware and Algorithm Co-design with Event Sensors.,CVPR Workshop on Event-based Vision,Oliver Cossairt,Northwestern Univ.,Generic introduction,Paper,"Abstract: In this talk I will provide an overview of our research developing hardware/software co-designs with event sensors, focusing on methods to fuse together information acquired from multiple sensing modalities for task-specific processing such as image reconstruction, object detection, and tracking. I will discuss three main thrusts of research, 1) extracting 3D information from event data using structured light, and inverse rendering, 2), fusing event sensor data together with conventional frame-based camera images, and 3) a feedback-driven, chip-host architecture built for lightweight on-camera processing equipped with novel data compression algorithms for high-bandwidth, task-specific chip/host communication. Finally, I will wrap up by briefly discussing our current work in-progress developing spiking-based neural network (SNN) algorithms to leverage similar co-design principles.",,,,,,,,
https://www.youtube.com/watch?v=BIvp86yFTYE&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=15,2021,Fusing Frame and Event data for High Dynamic Range Video,CVPR Workshop on Event-based Vision,Robert Mahony,Australian National University,Sensing,Paper,"Abstract: Event cameras produce data that is inherently high dynamic range and high temporal resolution but is not absolute, that is the actual intensity of a pixel is not directly available. Frame based cameras generate absolute intensity images but suffer from low dynamic range and are subject to low temporal image effects such as image blur. This talk discusses stochastic filtering algorithms for fusing these data sets to generate crisp high dynamic range images.",,,,,,,,
https://www.youtube.com/watch?v=0Zevb04mw3M&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=16,2021,Neuromorphic computing hardware and event based vision: a perfect match?,CVPR Workshop on Event-based Vision,Yulia Sandamirskaya,Intel,Generic introduction,Paper,"Abstract: I will provide an overview of spiking neuronal network based architecture for processing the event-based vision (EBV) output in neuromorphic hardware, highlighting examples of EBV workloads developed in Intel’s Neuromorphic Research Community.",,,,,,,,
https://www.youtube.com/watch?v=HiEkQO4PM0Y&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=17,2021,Neuromorphic vision for humanoid robots,CVPR Workshop on Event-based Vision,Chiara Bartolozzi,IIT,Robotics applications,Paper,"Abstract: Humanoid robots need to autonomously understand the world around them and be capable of planning appropriate and timely behaviour. Event-cameras can support the development of these capabilities, mostly thanks to their high temporal resolution, compressive encoding of visual data and high dynamic range. At the same time, new computational paradigms have to be devised to process the event-stream. These novel methods range from adapting ""traditional"" computer vision to run on events, to getting inspired from biological visual processing. In this presentation we will cover some examples of both approaches.",,,,,,,,
https://www.youtube.com/watch?v=eiIIhY7HeQM&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=18,2021,Back to Event Basics: Self-Supervised Learning of Image Reconstruction for Event Cameras via Photome,CVPR Workshop on Event-based Vision,F. Paredes-Vallés,TU Delft,Learning,Paper,"Reference: F. Paredes-Vallés and G. C. H. E. de Croon Back to Event Basics: Self-Supervised Learning of Image Reconstruction for Event Cameras via Photometric Constancy IEEE Conference on Computer Vision and Pattern Recognition, 2021. Project page: https://mavlab.tudelft.nl/ssl_e2v/ Code: https://mavlab.tudelft.nl/ssl_e2v/ Paper: https://arxiv.org/abs/2009.08283",,,,,,,,
https://www.youtube.com/watch?v=VzQ7b5-pLag&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=19,2021,v2e: From Video Frames to Realistic DVS Events,CVPR Workshop on Event-based Vision,Tobi Delbruck,INI Zurich,Software and tools,Paper,"Code: https://github.com/SensorsINI/v2e Paper: https://tub-rip.github.io/eventvision... Reference: Tobi Delbruck, Yuhuang Hu, Zhe He, v2e: From Video Frames to Realistic DVS Events,",,,,,,,,
https://www.youtube.com/watch?v=dVLyia-ezvo&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=20,2021,TimeLens: Event-based Video Frame Interpolation (CVPR 2021),CVPR Workshop on Event-based Vision,Stepan Tulyakov,INI Zurich,Software and tools,Paper,"Abstract: State-of-the-art frame interpolation methods generate intermediate frames by inferring object motions in the image from consecutive key-frames. In the absence of additional information, first-order approximations, i.e. optical flow, must be used, but this choice restricts the types of motions that can be modeled, leading to errors in highly dynamic scenarios. Event cameras are novel sensors that address this limitation by providing auxiliary visual information in the blind-time between frames. They asynchronously measure per-pixel brightness changes and do this with high temporal resolution and low latency. Event-based frame interpolation methods typically adopt a synthesis-based approach, where predicted frame residuals are directly applied to the key-frames. However, while these approaches can capture non-linear motions they suffer from ghosting and perform poorly in low-texture regions with few events. Thus, synthesis-based and flow-based approaches are complementary. In this work, we introduce Time Lens, a novel indicates equal contribution method that leverages the advantages of both. We extensively evaluate our method on three synthetic and two real benchmarks where we show an up to 5.21 dB improvement in terms of PSNR over state-of-the-art frame-based and event-based methods. Finally, we release a new large-scale dataset in highly dynamic scenarios, aimed at pushing the limits of existing methods.
",,,,,,,,
https://www.youtube.com/watch?v=S7ABoOMxytA&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=21,2021,Spatiotemporal registration for event-based visual odometry,CVPR Workshop on Event-based Vision,Daqi Liu,University of Adelaide,Sensing,ChatGPT,"The video is discussing a method for event-based visual odometry, which uses an event camera that detects intensity changes for every pixel on the image plane and generates an event when the difference is higher than a threshold. The method, called spatiotemporal registration (STR), utilizes a different philosophy from traditional methods, which warps all events in a batch to a reference time index with an initial guess motion parameter. STR splits the events into two sub-batches, warps events from the second batch to the first batch, and calculates the optimal motion parameter using SVD. The method is efficient, generates feature tracks, and is less affected by batch size. The video also compares STR with traditional methods and other algorithms for rotational-only visual odometry.",,,,,,,,
https://www.youtube.com/watch?v=KdpZkxjp02E&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=22,2021,0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera,CVPR Workshop on Event-based Vision,C.M. Parameshwara,University of Maryland,Sensing,Paper,"Segmentation of moving objects in dynamic scenes is a key process in scene understanding for navigation tasks. Classical cameras suffer from motion blur in such scenarios rendering them effete. On the contrary, event cameras, because of their high temporal resolution and lack of motion blur, are tailor-made for this problem. We present an approach for monocular multi-motion segmentation, which combines bottom-up feature tracking and top-down motion compensation into a unified pipeline, which is the first of its kind to our knowledge. Using the events within a time-interval, our method segments the scene into multiple motions by splitting and merging. We further speed up our method by using the concept of motion propagation and cluster keyslices. The approach was successfully evaluated on both challenging real-world and synthetic scenarios from the EV-IMO, EED, and MOD datasets and outperformed the state-of-the-art detection rate by 12%, achieving a new state-of-the-art average detection rate of 81.06%, 94.2%, and 82.35% on the aforementioned datasets. To enable further research and systematic evaluation of multi-motion segmentation, we present and open-source a new dataset/benchmark called MOD++, which includes challenging sequences and extensive data stratification in-terms of camera and object motion, velocity magnitudes, direction, and rotational speeds.",,,,,,,,
https://www.youtube.com/watch?v=QB0W3ZF08QE&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=23,2021,EVPropNet: Detecting Drones by Finding Propellers For Mid-Air Landing and Following,CVPR Workshop on Event-based Vision,Nitin J. Sanket,University of Maryland,Robotics applications,Paper,"In this paper, we model the geometry of a propeller and use it to generate simulated events which are used to train a deep neural network called EVPropNet to detect propellers from the data of an event camera. EVPropNet directly transfers to the real world without any fine-tuning or retraining. We present two applications of our network: (a) tracking and following an unmarked drone and (b) landing on a near-hover drone. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with different propeller shapes and sizes. Our network can detect propellers at a rate of 85.1% even when 60% of the propeller is occluded and can run at up to 35Hz on a 2W power budget. To our knowledge, this is the first deep learning-based solution for detecting propellers (to detect drones). Finally, our applications also show an impressive success rate of92% and 90% for the tracking and landing tasks respectively.",,,,,,,,
https://www.youtube.com/watch?v=MToCww2G14Y&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=24,2021,"Oculi SPU, Real-time Vision Intelligence at the Edge With Software-Defined Features",CVPR Workshop on Event-based Vision,Joe Malkian,Oculi,Neuromorphic platforms,Paper,"The technology presented in the video is a new architecture for artificial vision called the Oculi SPU (Sensing and Processing Unit), which is a complete vision solution on a single chip. The Oculi SPU is designed to deliver real-time vision intelligence at the edge with software-defined features, making it at least 30 times more efficient and significantly faster than existing alternatives. The Oculi SPU mimics the architecture of the human eye and brain, with parallel processing and a bi-directional link between the SPU and the outside world. The SPU also offers a lot of software-defined features and capabilities that can be tuned in real-time, making it the first practical silicon that closely mimics biology in terms of selectivity, parallel processing, and efficiency. The SPU is also sensor agnostic, meaning that it can work with a variety of sensors. Overall the Oculi SPU is seen as a new way to deliver bionic vision for machines.",,,,,,,,
https://www.youtube.com/watch?v=CoocgoUhxvU&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=25,2021,Image Reconstruction from Event Cameras using Laplacian-Prediction and Poisson Integration,CVPR Workshop on Event-based Vision,Hadar Cohen Duwek,The open university of israel,Event-based systems,Paper,"Event cameras are robust neuromorphic visual sensors, which communicate transients in luminance as events. Current paradigm for image reconstruction from event data relies on direct optimization of artificial Convolutional Neural Networks (CNNs). Here we proposed a two-phase neural network, which comprises a CNN, optimized for Laplacian prediction followed by a Spiking Neural Network (SNN) optimized for Poisson integration. By introducing Laplacian prediction into the pipeline, we provide image reconstruction with a network comprising only 200 parameters. We converted the CNN to SNN, providing a full neuromorphic implementation. We further optimized the network with Mish activation and a novel convoluted CNN design, proposing a hybrid of spiking and artificial neural network with < 100 parameters. Models were evaluated on both N-MNIST and N-Caltech101 datasets.",,,,,,,,
https://www.youtube.com/watch?v=Rrkwxp8J18c&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=26,2021,Detecting Stable Keypoints from Events through Image Gradient Prediction,CVPR Workshop on Event-based Vision,Philippe Chiberre,Prophesee,Event-based systems,Paper,"We present a method that detects stable keypoints from an event stream at high speed with a low memory footprint. Our key observation connects two points: It should be easier to reconstruct the image gradients rather than the image itself from the events, and the Harris corner detector, one of the most reliable keypoint detectors for short baseline regular images, depends on the image gradients, not the image. We therefore introduce a recurrent convolutional neural network to predict image gradients from events. As image gradients and events are correlated, this prediction task is relatively easy and we can keep this network very small. We train our network solely on synthetic data. Extracting Harris corners from these gradients is then very efficient. Moreover, in contrast to learned methods, we can change the hyperparameters of the detector without retraining. Our experiments confirm that predicting image gradients rather than images is much more efficient, and that our approach predicts stable corner points which are easier to track for a longer time compared to state-of-the-art event-based methods.",,,,,,,,
https://www.youtube.com/watch?v=O1ow7c_OlDQ&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=27,2021,Comparing Representations in Tracking for Event Camera-based SLAM,CVPR Workshop on Event-based Vision,J. Jiao,The Hong Kong University of Science and Technology,Event-based systems,ChatGPT,"In this video, the speaker, Jian Hojo, from the Hong Kong University of Science and Technology, discusses a comparison of different representations in tracking for event camera-based SLAM. The speaker explains that the first event camera only SLAM systems used three main programs: EKF-based tracking, depth estimation, and intensity map reconstruction. However, a real-time reconstruction requires a GPU. Hojo then explains that EBU is the first event camera-based SLAM system using a CPU, which employs plane-based methods for mapping and uses the event map for real-time tracking. Hojo then introduces a new method called ESV which proposed forward projection and nonlinear optimization using the spectrum timer constraints for stereo mapping and uses the time service map for tracking. Hojo presents results of the comparison of these two event representation, and an enhanced tracker by combining EM with TS with the degeneracy check. He hopes that this paper can indicate possible ways to improve the state of the art methods.",,,,,,,,
https://www.youtube.com/watch?v=dU2zv_Ggp7Y&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=28,2021,Differentiable Event Stream Simulator for Non-Rigid 3D Tracking,CVPR Workshop on Event-based Vision,Jalees Nehvi,Saarland University,Software and tools,Paper,"This paper introduces the first differentiable simulator of event streams, i.e., streams of asynchronous brightness change signals recorded by event cameras. Our differentiable simulator enables non-rigid 3D tracking of deformable objects (such as human hands, isometric surfaces and general watertight meshes) from event streams by leveraging an analysis-by-synthesis principle. So far, event-based tracking and reconstruction of non-rigid objects in 3D, like hands and body, has been either tackled using explicit event trajectories or large-scale datasets. In contrast, our method does not require any such processing or data, and can be readily applied to incoming event streams. We show the effectiveness of our approach for various types of non-rigid objects and compare to existing methods for non-rigid 3D tracking. In our experiments, the proposed energy-based formulations outperform competing RGB-based methods in terms of 3D errors. The source code and the new data are publicly available",,,,,,,,
https://www.youtube.com/watch?v=4wV0ejrcDo0&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=29,2021,DVS-OUTLAB: A Neuromorphic Event-Based Long Time Monitoring Dataset for Real-World Outdoor Scenarios,CVPR Workshop on Event-based Vision,Tobias Bolten,Hochschule Niederrhein University of Applied Sciences,Software and tools,Paper,"Neuromorphic vision sensors are biologically inspired devices which differ fundamentally from well known framebased sensors. Even though developments in this research area are increasing, applications that rely entirely on event cameras are still relatively rare. This becomes particularly clear when considering real outdoor scenarios apart from laboratory conditions. One obstacle to the development of event-based vision applications in this context may be the lack of labeled datasets for algorithm development and evaluation. Therefore we describe a recording setting of a DVS-based long time monitoring of an urban public area and provide labeled DVS data that also contain effects of environmental outdoor influences recorded in this process. We also describe the processing chain used for label generation, as well as results from a performed denoising benchmark utilizing various spatio-temporal event stream filters. The dataset contains almost 7 hours of real world outdoor event-data with ≈47k labeled regions of interest and can be downloaded at http://dnt.kr.hsnr.de/ DVS-OUTLAB/",,,,,,,,
https://www.youtube.com/watch?v=IOZl8MxrfpQ&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=30,2021,N-ROD: a Neuromorphic Dataset for ​Synthetic-to-Real Domain Adaptation,CVPR Workshop on Event-based Vision,M. Cannici,IIT,Software and tools,Paper,"Event cameras are novel neuromorphic sensors, which asynchronously capture pixel-level intensity changes in the form of “events”. Event simulation from existing RGB datasets is commonly used to overcome the need of large amount of annotated data, which lacks due to the novelty of the event sensors. In this context, the possibility of using event simulation in synthetic scenarios, where data generation is not limited to pre-existing datasets, is to date still unexplored. In this work, we analyze the synth-to-real domain shift in event data, i.e., the gap arising between simulated events obtained from synthetic renderings and those captured with a real camera on real images. To this purpose, we extend to the event modality the popular RGB-D Object Dataset (ROD), which already comes with its synthetic version (SynROD). The resulting Neuromorphic ROD dataset (N-ROD) is the first to enable a synth-to-real analysis on event data, showing the effectiveness of Domain Adaptation techniques in reducing the synth-to-real shift. Moreover, through extensive experiments on multi-modal RGBE data, we show that events can be effectively combined with conventional visual information, encouraging further research in this area. The N-ROD dataset is available at https://N-ROD-dataset.github.io/home/.",,,,,,,,
https://www.youtube.com/watch?v=hrfsDOL18pw&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=31,2021,EFI-Net: Video Frame Interpolation from Fusion of Events and Frames,CVPR Workshop on Event-based Vision,Genady Paikin,Samsung,Sensing,Paper,"Event cameras are sensors with pixels that respond independently and asynchronously to changes in scene illumination. Event cameras have a number of advantages when compared to conventional cameras: low-latency, high temporal resolution, high dynamic range, low power and sparse data output. However, existing event cameras also suffer from comparatively low spatial resolution and are sensitive to noise. Recently, it has been shown that it is possible to reconstruct an intensity frame stream from an event stream. These reconstructions preserve the high temporal rate of the event stream, but tend to suffer from significant artifacts and low image quality due to the shortcomings of event cameras. In this work we demonstrate that it is possible to combine the best of both worlds, by fusing a color frame stream at low temporal resolution and high spatial resolution with an event stream at high temporal resolution and low spatial resolution to generate a video stream with both high temporal and spatial resolutions while preserving the original color information. We utilize a novel event frame interpolation network (EFI-Net), a multi-phase convolutional neural network which fuses the frame and event streams. EFI-Net is trained using only simulated data and generalizes exceptionally well to real-world experimental data. We show that our method is able to interpolate frames where traditional video interpolation approaches fail, while also outperforming event-only reconstructions. We further contribute a new dataset, containing event camera data synchronized with high speed video. This work opens the door to a new application for event cameras, enabling high fidelity fusion with frame based image streams for generation of high-quality high-speed video. The dataset is available at https://drive.google.com/file/d/ 1UIGVBqNER_5KguYPAu5y7TVg-JlNhz3-/view? usp=sharing",,,,,,,,
https://www.youtube.com/watch?v=KyS_h8i9HpM&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=32,2021,A Cortically-inspired Architecture for Event-based Visual Motion Processing,CVPR Workshop on Event-based Vision,Francesca Peveri,Università degli Studi di Genova,Neuromorphic-compatible algorithms,Paper,"We developed and tested the architecture of a bioinspired Spiking Neural Network for motion estimation. The computation performed by the retina is emulated by the neuromorphic event-based image sensor DAVIS346 which constitutes the input of our network. We obtained neurons highly tuned to spatial frequency and orientation of the stimulus through a combination of feed-forward excitatory connections modeled as an elongated Gaussian kernel and recurrent inhibitory connections from two clusters of neurons within the same cortical layers. Sums over adjacent nodes weighted by time-variable synapses are used to attain Gabor-like spatio-temporal V1 receptive fields with selectivity to the stimulus’ motion. In order to gain the invariance to the stimulus phase, the two polarities of the events provided by the neuromorphic sensor were exploited, which allowed us to build two pairs of quadrature filters from which we obtain Motion Energy detectors as described in . Finally, a decoding stage allows us to compute optic flow from the Motion Detector layers. We tested the approach proposed with both synthetic and natural stimuli.",,,,,,,,
https://www.youtube.com/watch?v=TL567P70L68&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=33,2021,"Spike timing-based unsupervised learning of orientation, disparity, and motion representations SNN",CVPR Workshop on Event-based Vision,Thomas Barbier,CNRS,Learning,Paper,"Neuromorphic vision sensors present unique advantages over their frame based counterparts. However, unsupervised learning of efficient visual representations from their asynchronous output is still a challenge, requiring a rethinking of traditional image and video processing methods. Here we present a network of leaky integrate and fire neurons that learns representations similar to those of simple and complex cells in the primary visual cortex of mammals from the input of two event-based vision sensors. Through the combination of spike timing-dependent plasticity and homeostatic mechanisms, the network learns visual feature detectors for orientation, disparity, and motion in a fully unsupervised fashion. We validate our approach on a mobile robotic platform.",,,,,,,,
https://www.youtube.com/watch?v=nwWh3MKdOEM&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=34,2021,Lifting Monocular Events to 3D Human Poses,CVPR Workshop on Event-based Vision,Gianluca Scarpellini,IIT,Neuromorphic-compatible algorithms,Paper,"This paper presents a novel 3D human pose estimation approach using a single stream of asynchronous events as input. Most of the state-of-the-art approaches solve this task with RGB cameras, which suffer when the subjects are moving fast. On the other hand, event-based 3D pose estimation benefits from the advantages of event-cameras, especially their efficiency and robustness to appearance changes. Yet, finding human poses in asynchronous events is in general more challenging than standard RGB pose estimation, since little or no events are triggered in static scenes. Here we propose the first learning-based method for 3D human pose from a single stream of events. Our method consists of two steps. First, we process the event-camera stream to predict three orthogonal heatmaps per joint; each heatmap is the projection of of the joint onto one orthogonal plane. Next, we fuse the sets of heatmaps to estimate 3D localisation of the body joints. As a further contribution, we make available a new, challenging dataset for event-based human pose estimation by simulating events from the RGB Human3.6m dataset. Experiments demonstrate that our method achieves solid accuracy, narrowing the performance gap between standard RGB and event-based vision. The code is released at https://iit-pavis. github.io/lifting_events_to_3d_hpe.",,,,,,,,
https://www.youtube.com/watch?v=UFwgNsT-IVI&list=PLeXWz-g2If95mjNpA-y-WIoDaoB8WtmE7&index=35,2021,11 -Feedback control of event cameras - 3 minute teaser,CVPR Workshop on Event-based Vision,Tobi Delbruck,INI Zurich,Sensing,Paper,"Dynamic vision sensor event cameras produce a variable data rate stream of brightness change events. Event production at the pixel level is controlled by threshold, bandwidth, and refractory period bias current parameter settings. Biases must be adjusted to match application requirements and the optimal settings depend on many factors. As a first step towards automatic control of biases, this paper proposes fixed-step feedback controllers that use measurements of event rate and noise. The controllers regulate the event rate within an acceptable range using threshold and refractory period control, and regulate noise using bandwidth control. Experiments demonstrate model validity and feedback control.",,,,,,,,
